{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Classes Remapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to remap the classes of the Fakeddit dataset (subreddit sources) to our binary class problem (pristine or fake).\n",
    "In particular, we will consider as \"fake\" each image coming from the \"PS battle comments\" subreddit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from IPython.display import display\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the unique list of all the possible subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique subreddit values saved to subreddit_values.json\n",
      "['mildlyinteresting', 'pareidolia', 'neutralnews', 'photoshopbattles', 'nottheonion', 'psbattle_artwork', 'fakehistoryporn', 'propagandaposters', 'upliftingnews', 'fakealbumcovers', 'subredditsimulator', 'satire', 'savedyouaclick', 'misleadingthumbnails', 'pic', 'theonion', 'confusing_perspective', 'usnews', 'usanews', 'waterfordwhispersnews', 'subsimulatorgpt2', 'fakefacts']\n"
     ]
    }
   ],
   "source": [
    "multimodal_train_tsv_path = os.getenv('MULTIMODAL_TRAIN_CLEANED_NO_EXACT_DUPLICATES_NO_IMAGEONLY_DUPLICATES_TSV')\n",
    "\n",
    "df = pd.read_csv(multimodal_train_tsv_path, sep='\\t')\n",
    "\n",
    "#get unique values from the \"subreddit\" column\n",
    "unique_subreddits = list(df['subreddit'].unique())\n",
    "\n",
    "with open('subreddit_values.json', 'w') as json_file:\n",
    "    json.dump(unique_subreddits, json_file, indent=2)\n",
    "\n",
    "print(\"Unique subreddit values saved to subreddit_values.json\")\n",
    "print(unique_subreddits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add a column named \"class\" which will contain either \"fake\" or \"pristine\" depending on the \"subreddit\" column value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Counts:\n",
      "class\n",
      "pristine    385872\n",
      "fake        160261\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test Counts:\n",
      "class\n",
      "pristine    41567\n",
      "fake        16820\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation Counts:\n",
      "class\n",
      "pristine    40911\n",
      "fake        16912\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total \"pristine\": 468350 | Total \"fake\": 193993\n"
     ]
    }
   ],
   "source": [
    "multimodal_train_tsv_path = os.getenv('MULTIMODAL_TRAIN_CLEANED_NO_EXACT_DUPLICATES_NO_IMAGEONLY_DUPLICATES_TSV')\n",
    "multimodal_test_tsv_path = os.getenv('MULTIMODAL_TEST_CLEANED_NO_EXACT_DUPLICATES_TSV')\n",
    "multimodal_validation_tsv_path = os.getenv('MULTIMODAL_VAL_CLEANED_NO_EXACT_DUPLICATES_TSV')\n",
    "\n",
    "df_train = pd.read_csv(multimodal_train_tsv_path, sep='\\t')\n",
    "df_test = pd.read_csv(multimodal_test_tsv_path, sep='\\t')\n",
    "df_val = pd.read_csv(multimodal_validation_tsv_path, sep='\\t')\n",
    "\n",
    "#create a new column \"class\" and set default value to \"pristine\"\n",
    "df_train['class'] = 'pristine'\n",
    "df_test['class'] = 'pristine'\n",
    "df_val['class'] = 'pristine'\n",
    "\n",
    "#then set the value to \"fake\" for rows where \"subreddit\" is equal to \"psbattle_artwork\"\n",
    "df_train.loc[df_train['subreddit'] == 'psbattle_artwork', 'class'] = 'fake'\n",
    "df_test.loc[df_test['subreddit'] == 'psbattle_artwork', 'class'] = 'fake'\n",
    "df_val.loc[df_val['subreddit'] == 'psbattle_artwork', 'class'] = 'fake'\n",
    "\n",
    "df_train.to_csv(\"train_tsv_with_class.tsv\", sep='\\t', index=False)\n",
    "df_test.to_csv(\"test_tsv_with_class.tsv\", sep='\\t', index=False)\n",
    "df_val.to_csv(\"val_tsv_with_class.tsv\", sep='\\t', index=False)\n",
    "\n",
    "train_counts = df_train['class'].value_counts()\n",
    "test_counts = df_test['class'].value_counts()\n",
    "val_counts = df_val['class'].value_counts()\n",
    "\n",
    "print(\"Train Counts:\")\n",
    "print(train_counts)\n",
    "print(\"\\nTest Counts:\")\n",
    "print(test_counts)\n",
    "print(\"\\nValidation Counts:\")\n",
    "print(val_counts)\n",
    "\n",
    "print(\"\\n=> Total \\\"pristine\\\": \"+ str(train_counts['pristine']+test_counts['pristine']+val_counts['pristine'])+\" | Total \\\"fake\\\": \"+str(train_counts['fake']+test_counts['fake']+val_counts['fake']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
