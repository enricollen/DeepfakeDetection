{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nello\\AppData\\Local\\Temp\\ipykernel_20220\\3906296117.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Select the multimodal images only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather and merge all the multimodal images' names from multimodal_train.tsv, multimodal_test_public.tsv and multimodal_validate.tsv and export it to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_train_tsv_path = os.getenv('MULTIMODAL_TRAIN_TSV')\n",
    "multimodal_test_tsv_path = os.getenv('MULTIMODAL_TEST_TSV')\n",
    "multimodal_val_tsv_path = os.getenv('MULTIMODAL_VAL_TSV')\n",
    "\n",
    "#TSV files into pd\n",
    "train_df = pd.read_csv(multimodal_train_tsv_path, sep='\\t')\n",
    "test_df = pd.read_csv(multimodal_test_tsv_path, sep='\\t')\n",
    "validation_df = pd.read_csv(multimodal_val_tsv_path, sep='\\t')\n",
    "\n",
    "multimodal_images_names = list(set(train_df['id'].tolist() + test_df['id'].tolist() + validation_df['id'].tolist()))\n",
    "\n",
    "#export the list of all the multimodal images names to json\n",
    "with open(os.getenv('MULTIMODAL_IMAGES_JSON_PATH'), 'w') as json_file:\n",
    "    json.dump(multimodal_images_names, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete from the images dataset folder the ones that don't fall into the multimodal images json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.getenv('DATASET_PATH')\n",
    "multimodal_images_names_json_path = os.getenv('MULTIMODAL_IMAGES_JSON_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682661\n"
     ]
    }
   ],
   "source": [
    "with open(multimodal_images_names_json_path, 'r') as json_file:\n",
    "    allowed_images = set(json.load(json_file))\n",
    "print(len(allowed_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_non_multimodal_images(image_folder, json_file):\n",
    "    print(\"Deleting non-multimodal images...\")\n",
    "\n",
    "    all_images = {os.path.splitext(image)[0] for image in os.listdir(image_folder)}\n",
    "    images_to_delete = all_images - allowed_images\n",
    "    print(\"All images: \" + str(len(all_images)))\n",
    "    print(\"Multimodal images: \" + str(len(allowed_images)))\n",
    "    print(\"Found \" + str(len(images_to_delete)) + \" images to be deleted\")\n",
    "    \n",
    "    with tqdm(total=len(images_to_delete), desc=\"Deleting non-multimodal images\") as pbar:\n",
    "        for image_to_delete in images_to_delete:\n",
    "            os.remove(os.path.join(image_folder, image_to_delete + '.jpg'))\n",
    "            pbar.update(1)\n",
    "\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Cardinality before: 773563\n",
      "\n",
      "Deleting non-multimodal images...\n",
      "All images: 773563\n",
      "Multimodal images: 682661\n",
      "Found 90903 images to be deleted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting non-multimodal images: 100%|██████████| 90903/90903 [03:06<00:00, 487.13it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "\n",
      "Dataset Cardinality after: 682660\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Cardinality before: \" + str(len(os.listdir(dataset_path))) + \"\\n\")\n",
    "delete_non_multimodal_images(dataset_path, multimodal_images_names_json_path)\n",
    "print(\"\\nDataset Cardinality after: \" + str(len(os.listdir(dataset_path))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) \"Not Available\" images removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=108x54>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#image_not_found_sample = 'C:/Users/nello/Desktop/TESI_CODICE/dataset/dataset_small/1a5s5s.jpg'\n",
    "image_not_found_sample = 'C:/Users/nello/Desktop/TESI_CODICE/dataset/public_image_set/1a02u2.jpg'\n",
    "\n",
    "def display_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img.thumbnail((200, 200))\n",
    "    display(img)\n",
    "\n",
    "display_image(image_not_found_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_hash(filepath):\n",
    "    block_size = 65536\n",
    "    file_hash = hashlib.sha256()\n",
    "    with open(filepath, 'rb') as f:\n",
    "        for block in iter(lambda: f.read(block_size), b''):\n",
    "            file_hash.update(block)\n",
    "    return file_hash.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_delete_duplicate_images(given_image_path, folder_path):\n",
    "    given_image_hash = file_hash(given_image_path)\n",
    "    files_list = os.listdir(folder_path)\n",
    "\n",
    "    deleted_count = 0\n",
    "    not_found_images = []\n",
    "\n",
    "    for filename in tqdm(files_list, desc=\"Processing images\"):\n",
    "        if os.path.isfile(os.path.join(folder_path, filename)):\n",
    "            current_image_path = os.path.join(folder_path, filename)\n",
    "            current_image_hash = file_hash(current_image_path)\n",
    "\n",
    "            if current_image_hash == given_image_hash:\n",
    "                try:\n",
    "                    os.remove(current_image_path)\n",
    "                    deleted_count += 1\n",
    "                    not_found_images.append(os.path.splitext(filename)[0])\n",
    "                    #print(f\"Deleted: {current_image_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error deleting {current_image_path}: {e}\")\n",
    "    \n",
    "    with open(os.getenv('NOT_FOUND_IMAGES_JSON_PATH'), 'w') as json_file:\n",
    "        json.dump(not_found_images, json_file)\n",
    "\n",
    "    return deleted_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Cardinality before: 679783\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 679783/679783 [18:10<00:00, 623.40it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images deleted: 304\n",
      "\n",
      "Dataset Cardinality after: 679479\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Cardinality before: \" + str(len(os.listdir(dataset_path))) + \"\\n\")\n",
    "deleted_count = find_and_delete_duplicate_images(image_not_found_sample, dataset_path)\n",
    "print(f'Number of images deleted: {deleted_count}')\n",
    "print(\"\\nDataset Cardinality after: \" + str(len(os.listdir(dataset_path))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6946\n"
     ]
    }
   ],
   "source": [
    "def delete_images(delete_json_path, folder_path):\n",
    "    with open(delete_json_path, 'r') as json_file:\n",
    "        delete_list = json.load(json_file)\n",
    "    c=0\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        if file_name.endswith('.jpg') and file_name[:-4] in delete_list:\n",
    "            os.remove(file_path)\n",
    "            c+=1\n",
    "            #print(f\"Deleted: {file_path}\")\n",
    "    return c\n",
    "\n",
    "delete_json_path = 'C:/Users/nello/Desktop/TESI_CODICE/EDA/dataset_cleaning/delete.json'\n",
    "cc=delete_images(delete_json_path, dataset_path)\n",
    "print(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of images in the folder is: 672533\n"
     ]
    }
   ],
   "source": [
    "def count_images(folder_path):\n",
    "    with os.scandir(folder_path) as entries:\n",
    "        image_count = sum(1 for entry in entries if entry.is_file() and entry.name.lower().endswith('.jpg'))\n",
    "\n",
    "    return image_count\n",
    "\n",
    "num_images = count_images(dataset_path)\n",
    "\n",
    "print(f\"The number of images in the folder is: {num_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coherently update trains, test and val TSV files by deleting rows corresponding to the \"image not found\" images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multimodal_train_tsv_path = os.getenv('MULTIMODAL_TSV_PATH') + \"/multimodal_train_cleaned.tsv\"\n",
    "#multimodal_test_tsv_path = os.getenv('MULTIMODAL_TSV_PATH') + \"/multimodal_test_cleaned.tsv\"\n",
    "#multimodal_val_tsv_path = os.getenv('MULTIMODAL_TSV_PATH') + \"/multimodal_val_cleaned.tsv\"\n",
    "\n",
    "#TSV files into pd\n",
    "train_df = pd.read_csv(multimodal_train_tsv_path, sep='\\t')\n",
    "test_df = pd.read_csv(multimodal_test_tsv_path, sep='\\t')\n",
    "validation_df = pd.read_csv(multimodal_val_tsv_path, sep='\\t')\n",
    "\n",
    "with open(os.getenv('NOT_FOUND_IMAGES_JSON_PATH'), 'r') as json_file:\n",
    "    not_found_images = json.load(json_file)\n",
    "\n",
    "# delete rows based on the \"id\" column#\n",
    "train_df = train_df[~train_df['id'].isin(not_found_images)]\n",
    "test_df = test_df[~test_df['id'].isin(not_found_images)]\n",
    "validation_df = validation_df[~validation_df['id'].isin(not_found_images)]\n",
    "\n",
    "# convert back the filtered df to TSV files\n",
    "train_df.to_csv(os.getenv('MULTIMODAL_TSV_PATH') + \"/multimodal_train_cleaned.tsv\", sep='\\t', index=False)\n",
    "test_df.to_csv(os.getenv('MULTIMODAL_TSV_PATH') + \"/multimodal_test_cleaned.tsv\", sep='\\t', index=False)\n",
    "validation_df.to_csv(os.getenv('MULTIMODAL_TSV_PATH') + \"/multimodal_val_cleaned.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Exact Duplicate detection (same image + same text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate post detection (same image + same text) and deduplication (keeping only one post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a json file all_post_hash_x_title.json that contains for each post the image hash, the caption (title), the image id and the original set:\n",
    "```json\n",
    "{\"awxhir\": {\"hash\": \"1d3bee9c40\", \"title\": \"my walgreens offbrand mucinex was engraved\", \"set\": \"training\"}, ... }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_hash_dict(df, image_folder, set):\n",
    "    image_hash_dict = {}\n",
    "\n",
    "    #c=0\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc='Processing Images'):\n",
    "        #if c==2000:\n",
    "            #break\n",
    "        image_id = row['id']\n",
    "        image_title = row['clean_title']\n",
    "        image_path = os.path.join(image_folder, f\"{image_id}.jpg\")\n",
    "        #c+=1\n",
    "        if os.path.isfile(image_path):\n",
    "            hash_value = file_hash(image_path)\n",
    "            image_hash_dict[image_id] = {'hash': hash_value, 'title': image_title, 'set': set}\n",
    "        else:\n",
    "            print(f\"Image not found: {image_path}\")\n",
    "            continue\n",
    "        \n",
    "    return image_hash_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Image Hash Dictionaries:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image not found: C:/Users/nello/Desktop/TESI_CODICE/dataset/public_image_set\\crmnpjl.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 555695/555695 [17:03<00:00, 542.88it/s]\n",
      "Processing Images: 100%|██████████| 58411/58411 [02:55<00:00, 333.66it/s].83s/it]\n",
      "Processing Images: 100%|██████████| 58428/58428 [02:33<00:00, 381.56it/s]85s/it] \n",
      "Creating Image Hash Dictionaries: 100%|██████████| 3/3 [22:35<00:00, 451.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Results exported to all_posts_hash_x_title.json\n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=3, desc='Creating Image Hash Dictionaries') as pbar:\n",
    "    # image_x_hash dictionaries for each df\n",
    "    train_image_hash_dict = create_image_hash_dict(train_df, dataset_path, \"training\")\n",
    "    pbar.update(1) \n",
    "    test_image_hash_dict = create_image_hash_dict(test_df, dataset_path, \"test\")\n",
    "    pbar.update(1) \n",
    "    validation_image_hash_dict = create_image_hash_dict(validation_df, dataset_path, \"validation\")\n",
    "    pbar.update(1)  \n",
    "\n",
    "# merge dicts\n",
    "combined_image_hash_dict = {**train_image_hash_dict, **test_image_hash_dict, **validation_image_hash_dict}\n",
    "\n",
    "with open(os.getenv(\"POSTS_HASH_AND_TITLE_JSON_PATH\"), 'w') as json_file:\n",
    "    json.dump(combined_image_hash_dict, json_file)\n",
    "\n",
    "print(\"Done. Results exported to all_posts_hash_x_title.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "672533\n"
     ]
    }
   ],
   "source": [
    "with open(os.getenv('POSTS_HASH_AND_TITLE_JSON_PATH'), 'r') as json_file:\n",
    "    combined_image_hash_dict = json.load(json_file)\n",
    "print(len(combined_image_hash_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a json file exact_duplicates_images.json that contains all the posts that have same image hash + text:\n",
    " ```json\n",
    " {\n",
    "  \"332a1dde7933245_report reveals jesus christ have benefited from fathers\": {\n",
    "    \"hash\": \"332a1dde7933245\",\n",
    "    \"title\": \"report reveals jesus christ have benefited from fathers\",\n",
    "    \"duplicates\": [\n",
    "      {\n",
    "        \"image_id\": \"baydyy\",\n",
    "        \"image_set\": \"training\"\n",
    "      },\n",
    "      {\n",
    "        \"image_id\": \"b8694x\",\n",
    "        \"image_set\": \"test\"\n",
    "      }\n",
    "    ]\n",
    "  }, ... }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting Duplicates: 100%|██████████| 672533/672533 [00:02<00:00, 243041.12image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Results exported to exact_duplicates_image.json\n"
     ]
    }
   ],
   "source": [
    "with open(os.getenv(\"POSTS_HASH_AND_TITLE_JSON_PATH\"), \"r\") as json_file:\n",
    "    existing_data = json.load(json_file)\n",
    "\n",
    "exact_duplicates_dict = {} \n",
    "\n",
    "for image_id, info in tqdm(existing_data.items(), desc='Detecting exact duplicates', unit='image'):\n",
    "    hash_value = info['hash']\n",
    "    title = info['title']\n",
    "    image_set = info['set']\n",
    "\n",
    "    #get a unique identifier for each image composed of the hash and title\n",
    "    identifier = f\"{hash_value}_{title}\"\n",
    "\n",
    "    #check if the identifier is already encountered\n",
    "    if identifier in exact_duplicates_dict:\n",
    "        #here duplicate found, add the current image_id and set to the duplicates list\n",
    "        exact_duplicates_dict[identifier]['duplicates'].append({'image_id': image_id, 'image_set': image_set})\n",
    "    else:\n",
    "        #here instead i have first occurrence of this identifier, so i create a new entry\n",
    "        exact_duplicates_dict[identifier] = {'hash': hash_value, 'title': title, 'duplicates': [{'image_id': image_id, 'image_set': image_set}]}\n",
    "\n",
    "#filter out entries with only one image (no duplicates)\n",
    "exact_duplicates_dict = {key: value for key, value in exact_duplicates_dict.items() if len(value['duplicates']) > 1}\n",
    "\n",
    "with open(os.getenv(\"EXACT_DUPLICATE_IMAGE_JSON_PATH\"), 'w') as json_file:\n",
    "    json.dump(exact_duplicates_dict, json_file, indent=2)\n",
    "\n",
    "print(\"Done. Results exported to exact_duplicates_image.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3457\n"
     ]
    }
   ],
   "source": [
    "with open(os.getenv(\"EXACT_DUPLICATE_IMAGE_JSON_PATH\"), \"r\") as json_file:\n",
    "    exact_duplicates = json.load(json_file)\n",
    "print(len(exact_duplicates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now i create a json choosen_duplicate_to_maintain.json that contains only 1 post among all the duplicates, \n",
    "#### prioritizing the ones that comes from the test set over the training/validation set:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"332a1dde7933245_report reveals jesus christ have benefited from fathers\": {\n",
    "    \"hash\": \"332a1dde7933245d028e7f873480e1b1f8a5cbb496fc5f6c96d237982bd12853\",\n",
    "    \"title\": \"report reveals jesus christ have benefited from fathers\",\n",
    "    \"duplicates\": [\n",
    "      \"baydyy\"\n",
    "    ]\n",
    "  }, ... }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting Duplicates to maintain: 100%|██████████| 672533/672533 [00:02<00:00, 258353.97image/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Results exported to choosen_duplicates_to_maintain.json\n"
     ]
    }
   ],
   "source": [
    "with open(os.getenv(\"POSTS_HASH_AND_TITLE_JSON_PATH\"), \"r\") as json_file:\n",
    "    existing_data = json.load(json_file)\n",
    "\n",
    "exact_duplicates_dict = {} \n",
    "\n",
    "for image_id, info in tqdm(existing_data.items(), desc='Detecting Duplicates to maintain', unit='image'):\n",
    "    hash_value = info['hash']\n",
    "    title = info['title']\n",
    "    image_set = info['set']\n",
    "\n",
    "    # Create a unique identifier for each image based on hash and title\n",
    "    identifier = f\"{hash_value}_{title}\"\n",
    "\n",
    "    # Check if the identifier is already encountered\n",
    "    if identifier in exact_duplicates_dict:\n",
    "        # Duplicate found, add the current image_id to the duplicates list\n",
    "        exact_duplicates_dict[identifier]['duplicates'].append((image_id, image_set))\n",
    "    else:\n",
    "        # First occurrence of this identifier, create a new entry\n",
    "        exact_duplicates_dict[identifier] = {'hash': hash_value, 'title': title, 'duplicates': [(image_id, image_set)]}\n",
    "\n",
    "# Filter out entries with only one image (no duplicates)\n",
    "exact_duplicates_dict = {key: value for key, value in exact_duplicates_dict.items() if len(value['duplicates']) > 1} \n",
    "\n",
    "# Keep only one image from each set of duplicates, prioritize test set if present\n",
    "filtered_duplicates_dict = {}\n",
    "for identifier, data in exact_duplicates_dict.items():\n",
    "    duplicates = data['duplicates']\n",
    "    \n",
    "    # Check if there is at least one image from the test set\n",
    "    test_set_duplicates = [(image_id, image_set) for image_id, image_set in duplicates if image_set == 'test']\n",
    "    \n",
    "    if test_set_duplicates:\n",
    "        # Keep only one image from the test set\n",
    "        filtered_duplicates_dict[identifier] = {'hash': data['hash'], 'title': data['title'], 'duplicates': [test_set_duplicates[0][0]]}\n",
    "    else:\n",
    "        # If no test set images, keep one from training/validation set\n",
    "        filtered_duplicates_dict[identifier] = {'hash': data['hash'], 'title': data['title'], 'duplicates': [duplicates[0][0]]}\n",
    "\n",
    "with open(os.getenv(\"CHOOSEN_DUPLICATE_TO__MAINTAIN_JSON_PATH\"), 'w') as json_file:\n",
    "    json.dump(filtered_duplicates_dict, json_file, indent=2)\n",
    "\n",
    "print(f\"Done. Results exported to choosen_duplicates_to_maintain.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3457\n"
     ]
    }
   ],
   "source": [
    "with open(os.getenv(\"CHOOSEN_DUPLICATE_TO__MAINTAIN_JSON_PATH\"), \"r\") as json_file:\n",
    "    duplicates_to_maintain = json.load(json_file)\n",
    "print(len(duplicates_to_maintain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now i create another json duplicates_to_delete.json that contains the post ids to delete (not choosen):\n",
    "```json\n",
    "[\n",
    "  {\n",
    "    \"image_id\": \"b8694x\",\n",
    "    \"image_set\": \"training\"\n",
    "  },\n",
    "  {\n",
    "    \"image_id\": \"er6eggt\",\n",
    "    \"image_set\": \"training\"\n",
    "  }, ...\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3457\n",
      "3457\n"
     ]
    }
   ],
   "source": [
    "with open(os.getenv(\"EXACT_DUPLICATE_IMAGE_JSON_PATH\"), \"r\") as json_file:\n",
    "    exact_duplicates = json.load(json_file)\n",
    "print(len(exact_duplicates))\n",
    "\n",
    "with open(os.getenv(\"CHOOSEN_DUPLICATE_TO__MAINTAIN_JSON_PATH\"), \"r\") as json_file:\n",
    "    duplicates_to_maintain = json.load(json_file)\n",
    "print(len(duplicates_to_maintain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Results exported to duplicates_to_delete.json\n"
     ]
    }
   ],
   "source": [
    "duplicates_to_delete = []\n",
    "for key_b, value_b in exact_duplicates.items():\n",
    "    if key_b in duplicates_to_maintain:\n",
    "        duplicates_a = duplicates_to_maintain[key_b][\"duplicates\"]\n",
    "        duplicates_b = value_b[\"duplicates\"]\n",
    "        images_to_delete = [\n",
    "            {\"image_id\": item[\"image_id\"], \"image_set\": item[\"image_set\"]}\n",
    "            for item in duplicates_b\n",
    "            if item[\"image_id\"] not in duplicates_a\n",
    "        ]\n",
    "        duplicates_to_delete.extend(images_to_delete)\n",
    "\n",
    "with open(os.getenv(\"DUPLICATE_TO_DELETE_JSON_PATH\"), \"w\") as json_file:\n",
    "    json.dump(duplicates_to_delete, json_file, indent=2)\n",
    "\n",
    "print(f\"Done. Results exported to duplicates_to_delete.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3815\n"
     ]
    }
   ],
   "source": [
    "with open(os.getenv(\"DUPLICATE_TO_DELETE_JSON_PATH\"), \"r\") as json_file:\n",
    "    duplicates_to_delete = json.load(json_file)\n",
    "print(len(duplicates_to_delete))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that i have identified all the duplicate posts to delete, i can proceed to delete all the images that have the id inside duplicates_to_delete.json\n",
    "#### and to coherently delete the corresponding rows from the .tsv of train, test and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len before: 555695\n",
      "test len before: 58411\n",
      "val len before: 58428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting files: 100%|██████████| 3815/3815 [11:15<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len after: 552509\n",
      "test len after: 58387\n",
      "val len after: 57823\n",
      "3815 files deleted\n"
     ]
    }
   ],
   "source": [
    "multimodal_train_tsv_path = os.getenv('MULTIMODAL_TRAIN_CLEANED_TSV')\n",
    "multimodal_test_tsv_path = os.getenv('MULTIMODAL_TEST_CLEANED_TSV')\n",
    "multimodal_val_tsv_path = os.getenv('MULTIMODAL_VAL_CLEANED_TSV')\n",
    "\n",
    "train_df = pd.read_csv(multimodal_train_tsv_path, sep='\\t')\n",
    "test_df = pd.read_csv(multimodal_test_tsv_path, sep='\\t')\n",
    "validation_df = pd.read_csv(multimodal_val_tsv_path, sep='\\t')\n",
    "\n",
    "print(\"train len before: \" + str(len(train_df)))\n",
    "print(\"test len before: \" + str(len(test_df)))\n",
    "print(\"val len before: \" + str(len(validation_df)))\n",
    "\n",
    "c = 0\n",
    "\n",
    "for image_info in tqdm(duplicates_to_delete, desc=\"Deleting files\"):\n",
    "    image_id = image_info[\"image_id\"]\n",
    "    image_set = image_info[\"image_set\"]\n",
    "\n",
    "    file_path = os.path.join(dataset_path, f\"{image_id}.jpg\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        c += 1\n",
    "        #print(f\"Deleted file: {file_path}\")\n",
    "\n",
    "    #i update the corresponding dataframe based on image_set value\n",
    "    if image_set == \"training\":\n",
    "        train_df = train_df[train_df[\"id\"] != image_id]\n",
    "    elif image_set == \"test\":\n",
    "        test_df = test_df[test_df[\"id\"] != image_id]\n",
    "    elif image_set == \"validation\":\n",
    "        validation_df = validation_df[validation_df[\"id\"] != image_id]\n",
    "\n",
    "#convert back the filtered df to TSV files\n",
    "train_df.to_csv(os.getenv('MULTIMODAL_TRAIN_CLEANED_NO_EXACT_DUPLICATES_TSV'), sep='\\t', index=False)\n",
    "test_df.to_csv(os.getenv('MULTIMODAL_TEST_CLEANED_NO_EXACT_DUPLICATES_TSV'), sep='\\t', index=False)\n",
    "validation_df.to_csv(os.getenv('MULTIMODAL_VAL_CLEANED_NO_EXACT_DUPLICATES_TSV'), sep='\\t', index=False)\n",
    "\n",
    "print(\"train len after: \" + str(len(train_df)))\n",
    "print(\"test len after: \" + str(len(test_df)))\n",
    "print(\"val len after: \" + str(len(validation_df)))\n",
    "\n",
    "print(str(c) + \" files deleted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Cardinality after: 668718\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDataset Cardinality after: \" + str(len(os.listdir(dataset_path))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Image-only Duplicates Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We check whether there are images inside training set that have duplicate (image only) inside the test set and we delete it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "firstly, create a json file containing for each post in training set, its duplicates (if any) inside the test set, considering same image only and different title:\n",
    "```json\n",
    "{\n",
    "  \"5c61go\": [\n",
    "    \"53lt6a\",\n",
    "    \"5j4ikh\",\n",
    "    \"6174si\",\n",
    "    \"503e06\",\n",
    "    \"17n5ea\"\n",
    "  ], ...\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing train hashes:  74%|███████▍  | 409086/552509 [11:58<04:48, 497.57it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error computing hash for crmnpjl: [Errno 2] No such file or directory: 'C:/Users/nello/Desktop/TESI_CODICE/dataset/public_image_set\\\\crmnpjl.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing train hashes: 100%|██████████| 552509/552509 [15:54<00:00, 578.56it/s]\n",
      "Precomputing test hashes: 100%|██████████| 58387/58387 [01:27<00:00, 664.57it/s]\n",
      "Processing training set:  74%|███████▍  | 409095/552509 [22:41<07:55, 301.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash not found for crmnpjl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training set: 100%|██████████| 552509/552509 [30:39<00:00, 300.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates mapping saved to duplicates_mapping.json\n"
     ]
    }
   ],
   "source": [
    "multimodal_train_tsv_path = os.getenv('MULTIMODAL_TRAIN_CLEANED_NO_EXACT_DUPLICATES_TSV')\n",
    "multimodal_test_tsv_path = os.getenv('MULTIMODAL_TEST_CLEANED_NO_EXACT_DUPLICATES_TSV')\n",
    "\n",
    "train_df = pd.read_csv(multimodal_train_tsv_path, sep='\\t')\n",
    "test_df = pd.read_csv(multimodal_test_tsv_path, sep='\\t')\n",
    "\n",
    "duplicates_mapping = {}\n",
    "\n",
    "#precompute hashes for both training and test images\n",
    "train_hashes = {}\n",
    "for _, train_row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Precomputing train hashes\"):\n",
    "    train_image_id = train_row['id']\n",
    "    train_image_path = os.path.join(dataset_path, f\"{train_image_id}.jpg\")\n",
    "    try:\n",
    "        train_hashes[train_image_id] = file_hash(train_image_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing hash for {train_image_id}: {e}\")\n",
    "\n",
    "test_hashes = {}\n",
    "for _, test_row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Precomputing test hashes\"):\n",
    "    test_image_id = test_row['id']\n",
    "    test_image_path = os.path.join(dataset_path, f\"{test_image_id}.jpg\")\n",
    "    try:\n",
    "        test_hashes[test_image_id] = file_hash(test_image_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing hash for {test_image_id}: {e}\")\n",
    "\n",
    "for _, train_row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Processing training set\"):\n",
    "    train_image_id = train_row['id']\n",
    "    try:\n",
    "        train_image_hash = train_hashes[train_image_id]\n",
    "    except KeyError:\n",
    "        print(f\"Hash not found for {train_image_id}\")\n",
    "        continue \n",
    "\n",
    "    #check for duplicates in the test set using precomputed hashes\n",
    "    duplicates_in_test = [\n",
    "        test_image_id for test_image_id, test_hash in test_hashes.items()\n",
    "        if train_image_hash == test_hash\n",
    "    ]\n",
    "\n",
    "    #only add entries for training images with duplicates\n",
    "    if duplicates_in_test:\n",
    "        duplicates_mapping[train_image_id] = duplicates_in_test\n",
    "\n",
    "with open(os.getenv('IMAGE_ONLY_DUPLICATES_MAPPING_TRAIN_TEST_JSON_PATH'), \"w\") as json_file:\n",
    "    json.dump(duplicates_mapping, json_file, indent=2)\n",
    "\n",
    "print(\"Duplicates mapping saved to only_images_duplicates_mapping_train_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6376\n"
     ]
    }
   ],
   "source": [
    "print(len(duplicates_mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now i can delete the image-only duplicates in the training set, and maintain the ones in the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting training images: 100%|██████████| 668718/668718 [01:19<00:00, 8420.12it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 6376 images from training set\n",
      "Removed 6376 rows from training .tsv file\n"
     ]
    }
   ],
   "source": [
    "multimodal_train_tsv_path = os.getenv('MULTIMODAL_TRAIN_CLEANED_NO_EXACT_DUPLICATES_TSV')\n",
    "\n",
    "train_df = pd.read_csv(multimodal_train_tsv_path, sep='\\t')\n",
    "\n",
    "with open(os.getenv('IMAGE_ONLY_DUPLICATES_MAPPING_TRAIN_TEST_JSON_PATH'), \"r\") as json_file:\n",
    "    duplicates_mapping = json.load(json_file)\n",
    "\n",
    "unique_training_images = list(set(duplicates_mapping.keys()))\n",
    "\n",
    "#backup the list of all the duplicates of the training image that are going to be deleted in a json file\n",
    "with open(os.getenv('IMAGE_ONLY_DUPLICATES_DELETED_TRAINING'), \"w\") as json_file:\n",
    "    json.dump(unique_training_images, json_file, indent=2)\n",
    "\n",
    "c=0\n",
    "#delete training images and maintain only the duplicates in the test set (same images but different text)\n",
    "for filename in tqdm(os.listdir(dataset_path), desc=\"Deleting training images\"):\n",
    "    image_id, _ = os.path.splitext(filename)\n",
    "\n",
    "    if image_id in unique_training_images:\n",
    "        file_path = os.path.join(dataset_path, filename)\n",
    "        os.remove(file_path)\n",
    "        #print(f\"Deleted file: {file_path}\")\n",
    "        c+=1\n",
    "\n",
    "print(\"Deleted \"+str(c)+ \" images from training set\")\n",
    "\n",
    "#update the training.tsv file to remove rows corresponding to the deleted training images\n",
    "train_df = pd.read_csv(multimodal_train_tsv_path, sep='\\t')\n",
    "train_df = train_df[~train_df['id'].isin(unique_training_images)]\n",
    "train_df.to_csv(os.getenv(\"MULTIMODAL_TRAIN_CLEANED_NO_EXACT_DUPLICATES_NO_IMAGEONLY_DUPLICATES_TSV\"), sep='\\t', index=False)\n",
    "\n",
    "print(\"Removed \"+str(c)+ \" rows from training .tsv file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Cardinality after: 662342\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDataset Cardinality after: \" + str(len(os.listdir(dataset_path))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Delete all the corrupted images in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, the images that have too many megapixels (cause DecompressionBombError while processing) and images that are corrupted (cannot identify image file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "multimodal_train_tsv_path = os.getenv('MULTIMODAL_TRAIN_CLEANED_NO_EXACT_DUPLICATES_NO_IMAGEONLY_DUPLICATES_TSV')\n",
    "multimodal_test_tsv_path = os.getenv('MULTIMODAL_TEST_CLEANED_NO_EXACT_DUPLICATES_TSV')\n",
    "multimodal_validation_path = os.getenv('MULTIMODAL_VAL_CLEANED_NO_EXACT_DUPLICATES_TSV')\n",
    "\n",
    "train_df = pd.read_csv(multimodal_train_tsv_path, sep='\\t')\n",
    "test_df = pd.read_csv(multimodal_test_tsv_path, sep='\\t')\n",
    "val_df = pd.read_csv(multimodal_validation_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Cardinality: 546133\n",
      "test Cardinality: 58387\n",
      "val Cardinality: 57823\n",
      "\n",
      "Dataset images cardinality: 662343\n"
     ]
    }
   ],
   "source": [
    "print(f\"train Cardinality: {len(train_df)}\")\n",
    "print(f\"test Cardinality: {len(test_df)}\")\n",
    "print(f\"val Cardinality: {len(val_df)}\")\n",
    "\n",
    "print(\"\\nDataset images cardinality: \" + str(len(train_df)+len(test_df)+len(val_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = os.getenv('DATASET_DIR')\n",
    "CORRUPTED_IMAGES_JSON_PATH = os.getenv('CORRUPTED_IMAGES_JSON_PATH')\n",
    "MAX_MEGAPIXELS = 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, create a corrupted_images.json file that contains all the names of the images that have either too many pixels or that does not contain image file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "def calculate_max_dimensions(max_megapixels, aspect_ratio):\n",
    "    max_pixels = int(max_megapixels * 1e6)\n",
    "    height = int((max_pixels / aspect_ratio) ** 0.5)\n",
    "    width = int(aspect_ratio * height)\n",
    "    return width, height\n",
    "\n",
    "def delete_corrupted_images(input_folder, max_megapixels):\n",
    "\n",
    "    corrupted_images = []\n",
    "    num_images_removed = 0\n",
    "\n",
    "    filenames = os.listdir(input_folder)\n",
    "    for filename in tqdm(filenames, desc=\"Processing Images\", unit=\"image\"):\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "\n",
    "        try:\n",
    "            with Image.open(file_path) as img:\n",
    "                aspect_ratio = img.width / img.height\n",
    "                max_width, max_height = calculate_max_dimensions(max_megapixels, aspect_ratio)\n",
    "\n",
    "                # Check if the image exceeds the maximum size\n",
    "                if img.width > max_width or img.height > max_height:\n",
    "                    corrupted_images.append({\"filename\": os.path.splitext(filename)[0], \"error\": \"Size exceeds limit\"})\n",
    "                    num_images_removed += 1\n",
    "                    #os.remove(file_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            corrupted_images.append({\"filename\": os.path.splitext(filename)[0], \"error\": str(e).split(\"'\")[0]})\n",
    "            num_images_removed += 1\n",
    "            #os.remove(file_path) \n",
    "\n",
    "    # Export list of dictionaries to JSON\n",
    "    with open(CORRUPTED_IMAGES_JSON_PATH, \"w\") as json_file:\n",
    "        json.dump(corrupted_images, json_file, indent=2)\n",
    "\n",
    "    print(f\"Number of Images Removed: {num_images_removed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_corrupted_images(DATASET_DIR, MAX_MEGAPIXELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore the corrupted_images.tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_images_df = pd.read_json(CORRUPTED_IMAGES_JSON_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, delete coherently all the rows corresponding to the corrupted image names inside the DataFrames of train, test and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_corrupted_images_from_df(df, corrupted_images_df):\n",
    "    # Identify rows in the dataframe corresponding to corrupted images\n",
    "    corrupted_filenames = set(corrupted_images_df['filename'])\n",
    "    mask = df['id'].isin(corrupted_filenames)\n",
    "\n",
    "    # Remove corrupted rows from the dataframe\n",
    "    df_cleaned = df[~mask]\n",
    "\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTIMODAL_TRAIN_CLEANED_NO_EXACT_DUPLICATES_NO_IMAGEONLY_DUPLICATES_NO_CORRUPTED_TSV = os.getenv('MULTIMODAL_TRAIN_CLEANED_NO_EXACT_DUPLICATES_NO_IMAGEONLY_DUPLICATES_NO_CORRUPTED_TSV')\n",
    "MULTIMODAL_TEST_CLEANED_NO_EXACT_DUPLICATES_NO_CORRUPTED_TSV = os.getenv('MULTIMODAL_TEST_CLEANED_NO_EXACT_DUPLICATES_NO_CORRUPTED_TSV')\n",
    "MULTIMODAL_VAL_CLEANED_NO_EXACT_DUPLICATES_NO_CORRUPTED_TSV = os.getenv('MULTIMODAL_VAL_CLEANED_NO_EXACT_DUPLICATES_NO_CORRUPTED_TSV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_cleaned = delete_corrupted_images_from_df(train_df, corrupted_images_df)\n",
    "train_df_cleaned.to_csv(MULTIMODAL_TRAIN_CLEANED_NO_EXACT_DUPLICATES_NO_IMAGEONLY_DUPLICATES_NO_CORRUPTED_TSV, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_cleaned = delete_corrupted_images_from_df(test_df, corrupted_images_df)\n",
    "test_df_cleaned.to_csv(MULTIMODAL_TEST_CLEANED_NO_EXACT_DUPLICATES_NO_CORRUPTED_TSV, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_cleaned = delete_corrupted_images_from_df(val_df, corrupted_images_df)\n",
    "val_df_cleaned.to_csv(MULTIMODAL_VAL_CLEANED_NO_EXACT_DUPLICATES_NO_CORRUPTED_TSV, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the cardinalities after removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Cardinality: 542997\n",
      "test Cardinality: 58047\n",
      "val Cardinality: 57489\n",
      "\n",
      "Dataset images cardinality: 658533\n"
     ]
    }
   ],
   "source": [
    "print(f\"train Cardinality: {len(train_df_cleaned)}\")\n",
    "print(f\"test Cardinality: {len(test_df_cleaned)}\")\n",
    "print(f\"val Cardinality: {len(val_df_cleaned)}\")\n",
    "\n",
    "print(\"\\nDataset images cardinality: \" + str(len(train_df_cleaned)+len(test_df_cleaned)+len(val_df_cleaned)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Update the all_comments.tsv deleting the ones related to the removed posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nello\\AppData\\Local\\Temp\\ipykernel_20220\\1142979992.py:9: DtypeWarning: Columns (0,1,2,3,4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_comments_df = pd.read_csv(all_comments_tsv_path, sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "multimodal_train_tsv_path = os.getenv('MULTIMODAL_TRAIN_CLEANED_NO_EXACT_DUPLICATES_NO_IMAGEONLY_DUPLICATES_NO_CORRUPTED_TSV')\n",
    "multimodal_test_tsv_path = os.getenv('MULTIMODAL_TEST_CLEANED_NO_EXACT_DUPLICATES_NO_CORRUPTED_TSV')\n",
    "multimodal_validation_path = os.getenv('MULTIMODAL_VAL_CLEANED_NO_EXACT_DUPLICATES_NO_CORRUPTED_TSV')\n",
    "all_comments_tsv_path = os.getenv('ALL_COMMENTS_TSV')\n",
    "\n",
    "train_df = pd.read_csv(multimodal_train_tsv_path, sep='\\t')\n",
    "test_df = pd.read_csv(multimodal_test_tsv_path, sep='\\t')\n",
    "val_df = pd.read_csv(multimodal_validation_path, sep='\\t')\n",
    "all_comments_df = pd.read_csv(all_comments_tsv_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cardinality of comments .tsv before: 10697533\n",
      "Cardinality of comments .tsv after: 6132032\n",
      "Filtered comments saved to all_comments_cleaned.tsv\n"
     ]
    }
   ],
   "source": [
    "unique_ids = set(train_df['id']).union(set(test_df['id'])).union(set(val_df['id']))\n",
    "\n",
    "print(\"Cardinality of comments .tsv before:\", len(all_comments_df))\n",
    "\n",
    "#i keep only the rows in all_comments where submission_id is present in the unique IDs in the union of train, test and val\n",
    "filtered_comments_df = all_comments_df[all_comments_df['submission_id'].isin(unique_ids)]\n",
    "\n",
    "print(\"Cardinality of comments .tsv after:\", len(filtered_comments_df))\n",
    "\n",
    "# Save the filtered comments to a new TSV file\n",
    "filtered_comments_df.to_csv(os.getenv('ALL_COMMENTS_CLEANED_TSV'), sep='\\t', index=False)\n",
    "\n",
    "print(\"Filtered comments saved to all_comments_cleaned.tsv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
