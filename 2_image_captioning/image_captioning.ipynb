{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nello\\AppData\\Local\\Temp\\ipykernel_16000\\1398488637.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "c:\\Users\\nello\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nello\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Continue from cleaning phase, showing the cardinalities of the cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Counts:\n",
      "class\n",
      "pristine    385871\n",
      "fake        157126\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test Counts:\n",
      "class\n",
      "pristine    41567\n",
      "fake        16480\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation Counts:\n",
      "class\n",
      "pristine    40909\n",
      "fake        16580\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=> Total \"pristine\": 468347 | Total \"fake\": 190186\n"
     ]
    }
   ],
   "source": [
    "multimodal_train_tsv_path = os.getenv('MULTIMODAL_TRAIN_CLEANED_WITH_CLASS_TSV')\n",
    "multimodal_test_tsv_path = os.getenv('MULTIMODAL_TEST_CLEANED_WITH_CLASS_TSV')\n",
    "multimodal_validation_tsv_path = os.getenv('MULTIMODAL_VAL_CLEANED_WITH_CLASS_TSV')\n",
    "\n",
    "df_train = pd.read_csv(multimodal_train_tsv_path, sep='\\t')\n",
    "df_test = pd.read_csv(multimodal_test_tsv_path, sep='\\t')\n",
    "df_val = pd.read_csv(multimodal_validation_tsv_path, sep='\\t')\n",
    "\n",
    "train_counts = df_train['class'].value_counts()\n",
    "test_counts = df_test['class'].value_counts()\n",
    "val_counts = df_val['class'].value_counts()\n",
    "\n",
    "print(\"Train Counts:\")\n",
    "print(train_counts)\n",
    "print(\"\\nTest Counts:\")\n",
    "print(test_counts)\n",
    "print(\"\\nValidation Counts:\")\n",
    "print(val_counts)\n",
    "\n",
    "print(\"\\n=> Total \\\"pristine\\\": \"+ str(train_counts['pristine']+test_counts['pristine']+val_counts['pristine'])+\" | Total \\\"fake\\\": \"+str(train_counts['fake']+test_counts['fake']+val_counts['fake']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Select 250.000 pristine images to be captioned from train, test and val sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will select the first 20.000 from test, 20.000 from val, and 210.000 from train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected train pristine shape:  (210000, 17)\n",
      "Selected test pristine shape:  (20000, 17)\n",
      "Selected val pristine shape:  (20000, 17)\n"
     ]
    }
   ],
   "source": [
    "# Select the first 210,000 pristine images from the train set\n",
    "selected_train_pristine = df_train[df_train['class'] == 'pristine'].head(210000)\n",
    "\n",
    "# Select the first 20,000 pristine images from the test set\n",
    "selected_test_pristine = df_test[df_test['class'] == 'pristine'].head(20000)\n",
    "\n",
    "# Select the first 20,000 pristine images from the validation set\n",
    "selected_val_pristine = df_val[df_val['class'] == 'pristine'].head(20000)\n",
    "\n",
    "print(\"Selected train pristine shape: \", selected_train_pristine.shape)\n",
    "print(\"Selected test pristine shape: \", selected_test_pristine.shape)\n",
    "print(\"Selected val pristine shape: \", selected_val_pristine.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the selected pristine of each set to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_train_pristine.to_csv(\"csv/selected_train_pristine.csv\", index=False)\n",
    "selected_test_pristine.to_csv(\"csv/selected_test_pristine.csv\", index=False)\n",
    "selected_val_pristine.to_csv(\"csv/selected_val_pristine.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the ImageCaptioner class to be used to caption the pristine images from the sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioner:\n",
    "    def __init__(self, device=0, model=\"Salesforce/blip-image-captioning-base\"):\n",
    "        self.device = device\n",
    "        self.captioner = pipeline(\n",
    "            \"image-to-text\",\n",
    "            model=model,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    def process_images(self, image_paths, batch_size=500, error_file=\"error_images.txt\"):\n",
    "        total_inference_time = 0\n",
    "        result_dict = {\"image_name\": [], \"caption\": []}\n",
    "\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i + batch_size]\n",
    "\n",
    "            with tqdm(total=len(batch_paths),\n",
    "                    desc=f\"Processing Batch {i // batch_size + 1}/{len(image_paths) // batch_size}\", unit=\"image\") as pbar:\n",
    "                start_time = time.time()\n",
    "\n",
    "                try:\n",
    "                    captions = self.captioner(batch_paths, max_new_tokens=100)\n",
    "                except Exception as ex:\n",
    "                    if not os.path.exists(error_file):\n",
    "                        with open(error_file, \"w\"):\n",
    "                            pass \n",
    "\n",
    "                    with open(error_file, \"a\") as error_file_writer:\n",
    "                        error_file_writer.write(\"\\n\".join(batch_paths) + \"\\n\")\n",
    "                    pbar.update(len(batch_paths))\n",
    "                    continue  # Skip to the next iteration if an exception occurs\n",
    "\n",
    "                end_time = time.time()\n",
    "                batch_inference_time = end_time - start_time\n",
    "                total_inference_time += batch_inference_time\n",
    "\n",
    "                for path, caption in zip(batch_paths, captions):\n",
    "                    image_name = os.path.splitext(os.path.basename(path))[0]  # remove .jpg\n",
    "                    result_dict[\"image_name\"].append(image_name)\n",
    "                    result_dict[\"caption\"].append(caption[0][\"generated_text\"])\n",
    "                    pbar.update(1)\n",
    "\n",
    "        avg_time_per_image = total_inference_time / len(image_paths)\n",
    "        print(f\"\\nTotal Inference Time: {total_inference_time:.2f} seconds\")\n",
    "        print(f\"Avg Time Per Image Caption: {avg_time_per_image:.4f} seconds\")\n",
    "\n",
    "        result_df = pd.DataFrame(result_dict)\n",
    "        return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "captioner = ImageCaptioner(model=\"Salesforce/blip-image-captioning-large\")\n",
    "DATASET_DIR = os.getenv('DATASET_DIR')\n",
    "BATCH_SIZE = int(os.getenv('BATCH_SIZE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the method to be called onto one set at a time, to caption all the images of a set (e.g. training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_and_save_images(df, save_path):\n",
    "    # Extract paths of selected pristine images\n",
    "    pristine_paths = [os.path.join(DATASET_DIR, f\"{image_name}.jpg\") for image_name in df['id']]\n",
    "\n",
    "    # Process and caption images\n",
    "    generated_captions_df = captioner.process_images(image_paths=pristine_paths, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Merge the original captions from df with the generated captions\n",
    "    result_df = pd.merge(df, generated_captions_df, left_on=\"id\", right_on=\"image_name\", how=\"left\")\n",
    "\n",
    "    # Create a new DataFrame with 'id', 'original_caption', and 'caption'\n",
    "    result_df = result_df[['id', 'clean_title', 'caption']]\n",
    "\n",
    "    result_df = result_df.rename(columns={'clean_title': 'original_caption', 'caption': 'generated_caption'})\n",
    "    result_df.to_csv(save_path, index=False, header=True, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore the csv of the selected pristine for each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_train_pristine_csv_path = os.getenv('SELECTED_TRAIN_PRISTINE_CSV_PATH')\n",
    "selected_test_pristine_csv_path = os.getenv('SELECTED_TEST_PRISTINE_CSV_PATH')\n",
    "selected_validation_pristine_csv_path = os.getenv('SELECTED_VAL_PRISTINE_CSV_PATH')\n",
    "\n",
    "selected_train_pristine = pd.read_csv(selected_train_pristine_csv_path, sep=',')\n",
    "selected_test_pristine = pd.read_csv(selected_test_pristine_csv_path, sep=',')\n",
    "selected_val_pristine = pd.read_csv(selected_validation_pristine_csv_path, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caption 210.000 images from training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_and_save_images(selected_train_pristine, \"csv/training_pristine_captioned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caption 20.000 images from test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 1/40: 100%|██████████| 500/500 [01:32<00:00,  5.38image/s] \n",
      "Processing Batch 2/40: 100%|██████████| 500/500 [01:32<00:00,  5.39image/s] \n",
      "Processing Batch 3/40:   0%|          | 0/500 [00:00<?, ?image/s]c:\\Users\\nello\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Processing Batch 3/40: 100%|██████████| 500/500 [01:33<00:00,  5.37image/s] \n",
      "Processing Batch 4/40: 100%|██████████| 500/500 [01:34<00:00,  5.30image/s] \n",
      "Processing Batch 5/40: 100%|██████████| 500/500 [01:33<00:00,  5.35image/s] \n",
      "Processing Batch 6/40: 100%|██████████| 500/500 [01:34<00:00,  5.27image/s] \n",
      "Processing Batch 7/40: 100%|██████████| 500/500 [01:35<00:00,  5.22image/s] \n",
      "Processing Batch 8/40: 100%|██████████| 500/500 [01:35<00:00,  5.25image/s] \n",
      "Processing Batch 9/40: 100%|██████████| 500/500 [01:35<00:00,  5.25image/s] \n",
      "Processing Batch 10/40: 100%|██████████| 500/500 [01:40<00:00,  4.97image/s]  \n",
      "Processing Batch 11/40:   0%|          | 0/500 [00:00<?, ?image/s]c:\\Users\\nello\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Processing Batch 11/40: 100%|██████████| 500/500 [01:35<00:00,  5.23image/s] \n",
      "Processing Batch 12/40: 100%|██████████| 500/500 [01:35<00:00,  5.22image/s] \n",
      "Processing Batch 13/40: 100%|██████████| 500/500 [01:35<00:00,  5.26image/s] \n",
      "Processing Batch 14/40: 100%|██████████| 500/500 [01:34<00:00,  5.31image/s] \n",
      "Processing Batch 15/40: 100%|██████████| 500/500 [01:35<00:00,  5.23image/s] \n",
      "Processing Batch 16/40: 100%|██████████| 500/500 [01:28<00:00,  5.65image/s] \n",
      "Processing Batch 17/40: 100%|██████████| 500/500 [01:26<00:00,  5.79image/s] \n",
      "Processing Batch 18/40: 100%|██████████| 500/500 [01:21<00:00,  6.13image/s] \n",
      "Processing Batch 19/40: 100%|██████████| 500/500 [01:23<00:00,  5.99image/s] \n",
      "Processing Batch 20/40: 100%|██████████| 500/500 [01:19<00:00,  6.27image/s] \n",
      "Processing Batch 21/40: 100%|██████████| 500/500 [01:29<00:00,  5.59image/s] \n",
      "Processing Batch 22/40: 100%|██████████| 500/500 [01:28<00:00,  5.65image/s] \n",
      "Processing Batch 23/40: 100%|██████████| 500/500 [01:50<00:00,  4.51image/s]  \n",
      "Processing Batch 24/40: 100%|██████████| 500/500 [02:28<00:00,  3.37image/s]  \n",
      "Processing Batch 25/40: 100%|██████████| 500/500 [01:54<00:00,  4.38image/s]  \n",
      "Processing Batch 26/40: 100%|██████████| 500/500 [01:22<00:00,  6.09image/s] \n",
      "Processing Batch 27/40: 100%|██████████| 500/500 [01:21<00:00,  6.15image/s] \n",
      "Processing Batch 28/40: 100%|██████████| 500/500 [01:27<00:00,  5.74image/s] \n",
      "Processing Batch 29/40: 100%|██████████| 500/500 [01:25<00:00,  5.87image/s] \n",
      "Processing Batch 30/40: 100%|██████████| 500/500 [01:24<00:00,  5.95image/s] \n",
      "Processing Batch 31/40: 100%|██████████| 500/500 [01:22<00:00,  6.06image/s] \n",
      "Processing Batch 32/40: 100%|██████████| 500/500 [01:23<00:00,  5.99image/s] \n",
      "Processing Batch 33/40: 100%|██████████| 500/500 [01:22<00:00,  6.05image/s] \n",
      "Processing Batch 34/40: 100%|██████████| 500/500 [01:20<00:00,  6.20image/s] \n",
      "Processing Batch 35/40: 100%|██████████| 500/500 [01:21<00:00,  6.14image/s] \n",
      "Processing Batch 36/40: 100%|██████████| 500/500 [01:22<00:00,  6.04image/s] \n",
      "Processing Batch 37/40: 100%|██████████| 500/500 [01:25<00:00,  5.82image/s] \n",
      "Processing Batch 38/40: 100%|██████████| 500/500 [01:20<00:00,  6.23image/s] \n",
      "Processing Batch 39/40: 100%|██████████| 500/500 [01:21<00:00,  6.14image/s] \n",
      "Processing Batch 40/40: 100%|██████████| 500/500 [01:19<00:00,  6.26image/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Inference Time: 3635.98 seconds\n",
      "Avg Time Per Image Caption: 0.1818 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "caption_and_save_images(selected_test_pristine, \"csv/test_pristine_captioned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caption 20.000 images from validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batch 1/40: 100%|██████████| 500/500 [01:29<00:00,  5.58image/s] \n",
      "Processing Batch 2/40: 100%|██████████| 500/500 [01:24<00:00,  5.92image/s] \n",
      "Processing Batch 3/40: 100%|██████████| 500/500 [01:28<00:00,  5.64image/s] \n",
      "Processing Batch 4/40: 100%|██████████| 500/500 [01:29<00:00,  5.58image/s] \n",
      "Processing Batch 5/40: 100%|██████████| 500/500 [01:27<00:00,  5.70image/s] \n",
      "Processing Batch 6/40: 100%|██████████| 500/500 [01:25<00:00,  5.86image/s] \n",
      "Processing Batch 7/40: 100%|██████████| 500/500 [01:27<00:00,  5.72image/s] \n",
      "Processing Batch 8/40: 100%|██████████| 500/500 [01:27<00:00,  5.70image/s] \n",
      "Processing Batch 9/40: 100%|██████████| 500/500 [01:26<00:00,  5.79image/s] \n",
      "Processing Batch 10/40: 100%|██████████| 500/500 [01:25<00:00,  5.82image/s] \n",
      "Processing Batch 11/40:   0%|          | 0/500 [00:00<?, ?image/s]c:\\Users\\nello\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Processing Batch 11/40: 100%|██████████| 500/500 [01:29<00:00,  5.59image/s] \n",
      "Processing Batch 12/40:   0%|          | 0/500 [00:00<?, ?image/s]c:\\Users\\nello\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Processing Batch 12/40: 100%|██████████| 500/500 [01:28<00:00,  5.64image/s] \n",
      "Processing Batch 13/40: 100%|██████████| 500/500 [01:27<00:00,  5.73image/s] \n",
      "Processing Batch 14/40: 100%|██████████| 500/500 [01:27<00:00,  5.70image/s] \n",
      "Processing Batch 15/40: 100%|██████████| 500/500 [01:29<00:00,  5.59image/s] \n",
      "Processing Batch 16/40: 100%|██████████| 500/500 [01:27<00:00,  5.69image/s] \n",
      "Processing Batch 17/40: 100%|██████████| 500/500 [01:27<00:00,  5.70image/s] \n",
      "Processing Batch 18/40: 100%|██████████| 500/500 [01:27<00:00,  5.73image/s] \n",
      "Processing Batch 19/40: 100%|██████████| 500/500 [01:28<00:00,  5.66image/s] \n",
      "Processing Batch 20/40: 100%|██████████| 500/500 [01:27<00:00,  5.73image/s] \n",
      "Processing Batch 21/40: 100%|██████████| 500/500 [01:28<00:00,  5.62image/s] \n",
      "Processing Batch 22/40: 100%|██████████| 500/500 [01:27<00:00,  5.73image/s] \n",
      "Processing Batch 23/40: 100%|██████████| 500/500 [01:29<00:00,  5.61image/s] \n",
      "Processing Batch 24/40: 100%|██████████| 500/500 [01:26<00:00,  5.76image/s] \n",
      "Processing Batch 25/40: 100%|██████████| 500/500 [01:27<00:00,  5.72image/s] \n",
      "Processing Batch 26/40: 100%|██████████| 500/500 [01:27<00:00,  5.74image/s] \n",
      "Processing Batch 27/40: 100%|██████████| 500/500 [01:27<00:00,  5.73image/s] \n",
      "Processing Batch 28/40: 100%|██████████| 500/500 [01:24<00:00,  5.90image/s] \n",
      "Processing Batch 29/40: 100%|██████████| 500/500 [01:26<00:00,  5.80image/s] \n",
      "Processing Batch 30/40: 100%|██████████| 500/500 [01:53<00:00,  4.40image/s]  \n",
      "Processing Batch 31/40: 100%|██████████| 500/500 [01:27<00:00,  5.71image/s] \n",
      "Processing Batch 32/40: 100%|██████████| 500/500 [01:25<00:00,  5.84image/s] \n",
      "Processing Batch 33/40: 100%|██████████| 500/500 [01:26<00:00,  5.76image/s] \n",
      "Processing Batch 34/40: 100%|██████████| 500/500 [01:26<00:00,  5.81image/s] \n",
      "Processing Batch 35/40: 100%|██████████| 500/500 [01:29<00:00,  5.59image/s] \n",
      "Processing Batch 36/40: 100%|██████████| 500/500 [01:26<00:00,  5.81image/s] \n",
      "Processing Batch 37/40: 100%|██████████| 500/500 [01:27<00:00,  5.69image/s] \n",
      "Processing Batch 38/40: 100%|██████████| 500/500 [01:27<00:00,  5.70image/s] \n",
      "Processing Batch 39/40: 100%|██████████| 500/500 [01:32<00:00,  5.41image/s] \n",
      "Processing Batch 40/40: 100%|██████████| 500/500 [01:27<00:00,  5.70image/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Inference Time: 3529.75 seconds\n",
      "Avg Time Per Image Caption: 0.1765 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "caption_and_save_images(selected_val_pristine, \"csv/validation_pristine_captioned.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
