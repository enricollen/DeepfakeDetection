{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Type 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN CLASS 0: collect the 55k fakes already generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55610\n",
      "Filtered IDs saved to train_class_0_already_generated.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('C:/Users/nello/Documents/vscode_projects/Thesis/3_image_generation/generated_images/csv/train_synthetics.csv')\n",
    "\n",
    "# Filter rows where '6_way_label' equals 0 and 'fake_id' starts with 'SD', 'DL', or 'GL'\n",
    "filtered_rows_SD = df[(df['6_way_label'] == 0) & (df['fake_id'].str.startswith('SD'))]\n",
    "filtered_rows_DL = df[(df['6_way_label'] == 0) & (df['fake_id'].str.startswith('DL'))]\n",
    "filtered_rows_GL = df[(df['6_way_label'] == 0) & (df['fake_id'].str.startswith('GL'))]\n",
    "\n",
    "# Concatenate columns for all groups\n",
    "result = pd.concat([filtered_rows_SD, filtered_rows_DL, filtered_rows_GL])\n",
    "print(len(result))\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "result.to_csv('csv/train_class_0_already_generated.csv', index=False)\n",
    "\n",
    "print(\"Filtered IDs saved to train_class_0_already_generated.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save a csv for the 107k pristine and the 107k-55k to be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train_class_0_already_generated: 55610\n",
      "len of train_pristine: 107548\n",
      "len of train_class_0_to_be_generated: 51939\n",
      "train_class_0_already_generated: 55.610 + train_class_0_to_be_generated: 51.938 = 107548\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "MULTIMODAL_TRAIN_CLEANED_WITH_CLASS_TSV = \"C:/Users/nello/OneDrive - University of Pisa/TESI/TSV_JSON/1_dataset_cleaning/tsv/train_tsv_with_class.tsv\"\n",
    "ALREADY_GENERATED_FROM_CLASS_0 = \"csv/train_class_0_already_generated.csv\"\n",
    "\n",
    "train = pd.read_csv(MULTIMODAL_TRAIN_CLEANED_WITH_CLASS_TSV, sep='\\t')\n",
    "train_filtered = train[train['6_way_label'] == 0]\n",
    "\n",
    "train_class_0_already_generated = pd.read_csv(ALREADY_GENERATED_FROM_CLASS_0)\n",
    "print(\"len of train_class_0_already_generated: \"+str(len(train_class_0_already_generated)))\n",
    "\n",
    "# Filter rows from train DataFrame that are not in the 'id' column of the other DataFrame\n",
    "train_not_in_other = train_filtered[~train_filtered['id'].isin(train_class_0_already_generated['id'])]\n",
    "\n",
    "# Select the first 107,549 rows from train_not_in_other\n",
    "train_pristine = train_not_in_other.head(107548)\n",
    "\n",
    "# Save train_pristine to CSV\n",
    "print(\"len of train_class_0_pristine: \"+str(len(train_pristine)))\n",
    "train_pristine.to_csv('csv/train_class_0_pristine.csv', index=False)\n",
    "\n",
    "# Filter rows from train_filtered that are not in train_class_0_already_generated or train_pristine\n",
    "train_remaining = train_filtered[~train_filtered['id'].isin(train_class_0_already_generated['id']) & ~train_filtered['id'].isin(train_pristine['id'])]\n",
    "\n",
    "# Save the remaining rows to a new CSV file\n",
    "print(\"len of train_class_0_to_be_generated: \"+str(len(train_remaining)))\n",
    "train_remaining.to_csv('csv/train_class_0_to_be_generated.csv', index=False)\n",
    "print(\"train_class_0_already_generated: 55.610 + train_class_0_to_be_generated: 51.938 = \" + str(55610+51938))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rename the columns real_image and fake_image to pristine_image and generated_image inside train_class_0_already_generated.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/train_class_0_already_generated.csv')\n",
    "\n",
    "# Rename the column\n",
    "df.rename(columns={\"real_image\": \"pristine_image\"}, inplace=True)\n",
    "df.rename(columns={\"fake_image\": \"generated_image\"}, inplace=True)\n",
    "df.to_csv(\"csv/train_class_0_already_generated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also change the value of columns real_image and fake_image in train_class_0_already_generated.csv from real_image=1 and fake_image=0 to real_image=0 and fake_image=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/train_class_0_already_generated.csv')\n",
    "\n",
    "# Swap the values of the two columns\n",
    "df['pristine_image'], df['generated_image'] = df['generated_image'], df['pristine_image']\n",
    "\n",
    "df.to_csv(\"csv/train_class_0_already_generated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduce columns of train_class_0_to_be_generated and train_class_0_pristine csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('csv/train_class_0_to_be_generated.csv')\n",
    "\n",
    "# Rename the column\n",
    "df.rename(columns={\"clean_title\": \"original_caption\"}, inplace=True)\n",
    "\n",
    "df['pristine_image'] = 1\n",
    "df['generated_image'] = 0\n",
    "df['real_text'] = 1\n",
    "df['fakenews_text'] = 0\n",
    "\n",
    "# Select only the desired columns in the required order\n",
    "desired_columns = ['id', 'author', 'original_caption', 'num_comments', '6_way_label', 'pristine_image', 'generated_image', 'real_text', 'fakenews_text']\n",
    "df = df[desired_columns]\n",
    "\n",
    "df.to_csv(\"csv/train_class_0_to_be_generated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('csv/train_class_0_pristine.csv')\n",
    "\n",
    "# Rename the column\n",
    "df.rename(columns={\"clean_title\": \"original_caption\"}, inplace=True)\n",
    "\n",
    "df['pristine_image'] = 1\n",
    "df['generated_image'] = 0\n",
    "df['real_text'] = 1\n",
    "df['fakenews_text'] = 0\n",
    "\n",
    "# Select only the desired columns in the required order\n",
    "desired_columns = ['id', 'author', 'original_caption', 'num_comments', '6_way_label', 'pristine_image', 'generated_image', 'real_text', 'fakenews_text']\n",
    "df = df[desired_columns]\n",
    "\n",
    "df.to_csv(\"csv/train_class_0_pristine.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if every id inside the 250k train_tsv_with_class.csv is contained in the union of the 3 csv train_pristine + train_class_0_already_generated + train_class_0_to_be_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215097\n",
      "215097\n",
      "The union of IDs from the first three CSVs corresponds exactly to the IDs from train_tsv_with_class.csv where '6_way_label' = 0.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the 'id' column from the first three CSV files into sets\n",
    "csv1_ids = set(pd.read_csv(\"csv/train_pristine.csv\")['id'])\n",
    "csv2_ids = set(pd.read_csv(\"csv/train_class_0_to_be_generated.csv\")['id'])\n",
    "csv3_ids = set(pd.read_csv(\"csv/train_class_0_already_generated.csv\")['id'])\n",
    "\n",
    "# Find the union of these three sets\n",
    "union_ids = csv1_ids.union(csv2_ids, csv3_ids)\n",
    "\n",
    "csv4_ids = set(pd.read_csv(\"C:/Users/nello/OneDrive - University of Pisa/TESI/TSV_JSON/1_dataset_cleaning/tsv/train_tsv_with_class.tsv\", sep='\\t').loc[pd.read_csv(\"C:/Users/nello/OneDrive - University of Pisa/TESI/TSV_JSON/1_dataset_cleaning/tsv/train_tsv_with_class.tsv\", sep='\\t')['6_way_label'] == 0, 'id'])\n",
    "\n",
    "print(len(union_ids))\n",
    "print(len(csv4_ids))\n",
    "\n",
    "# Check if the union_ids set is equal to the csv4_ids set\n",
    "if union_ids == csv4_ids:\n",
    "    print(\"The union of IDs from the first three CSVs corresponds exactly to the IDs from train_tsv_with_class.csv where '6_way_label' = 0.\")\n",
    "else:\n",
    "    print(\"The union of IDs from the first three CSVs does not correspond exactly to the IDs from train_tsv_with_class.csv where '6_way_label' = 0.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now i print out how many StableDiffusion, Dreamlike and Glide images i have already generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where 'fake_id' starts with 'SD': 22350\n",
      "Number of rows where 'fake_id' starts with 'DL': 22044\n",
      "Number of rows where 'fake_id' starts with 'GL': 11216\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"csv/train_class_0_already_generated.csv\")\n",
    "\n",
    "# Count the rows where the 'fake_id' column starts with 'SD', 'DL', or 'GL'\n",
    "count_SD = len(df[df['fake_id'].str.startswith('SD')])\n",
    "count_DL = len(df[df['fake_id'].str.startswith('DL')])\n",
    "count_GL = len(df[df['fake_id'].str.startswith('GL')])\n",
    "\n",
    "print(\"Number of rows where 'fake_id' starts with 'SD':\", count_SD)\n",
    "print(\"Number of rows where 'fake_id' starts with 'DL':\", count_DL)\n",
    "print(\"Number of rows where 'fake_id' starts with 'GL':\", count_GL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, to get to 107.549 generated images i want to generate other:\n",
    "\n",
    "- SD: 40.000 - 22.350 = 17.650\n",
    "- DL: 40.000 - 22.044 = 17.956\n",
    "- GL: (107.549 - 80.000) - 11.216 = 16.333\n",
    "\n",
    "so 51.939 in total from the train_class_0_to_be_generated.csv, but first i need to caption all the images from train_class_0_to_be_generated.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that every pristine image inside train_class_0_to_be_generated csv is already in the image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching images: 51939\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the 'id' column from the CSV file into a set\n",
    "csv_ids = set(pd.read_csv(\"csv/train/train_class_0_to_be_generated.csv\")['id'])\n",
    "\n",
    "# Get a list of all the image filenames in the folder\n",
    "folder_path = \"/home/enriconello/DeepFakeDetection/dataset\"\n",
    "folder_images = set(filename.split('.')[0] for filename in os.listdir(folder_path))\n",
    "\n",
    "# Find the intersection to identify matching images\n",
    "matching_images = csv_ids.intersection(folder_images)\n",
    "\n",
    "# Count the number of matching images\n",
    "num_matching_images = len(matching_images)\n",
    "\n",
    "print(\"Number of matching images:\", num_matching_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that every pristine image inside train_class_0_pristine csv is already in the image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching images: 107548\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the 'id' column from the CSV file into a set\n",
    "csv_ids = set(pd.read_csv(\"csv/train/train_class_0_pristine.csv\")['id'])\n",
    "\n",
    "# Get a list of all the image filenames in the folder\n",
    "folder_path = \"/home/enriconello/DeepFakeDetection/dataset\"\n",
    "folder_images = set(filename.split('.')[0] for filename in os.listdir(folder_path))\n",
    "\n",
    "# Find the intersection to identify matching images\n",
    "matching_images = csv_ids.intersection(folder_images)\n",
    "\n",
    "# Count the number of matching images\n",
    "num_matching_images = len(matching_images)\n",
    "\n",
    "print(\"Number of matching images:\", num_matching_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that every pristine image inside train_class_0_already_generated csv is already in the image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching images: 55610\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the 'id' column from the CSV file into a set\n",
    "csv_ids = set(pd.read_csv(\"csv/train/train_class_0_already_generated.csv\")['fake_id'])\n",
    "\n",
    "# Get a list of all the image filenames in the folder\n",
    "folder_path = \"/home/enriconello/DeepFakeDetection/dataset\"\n",
    "folder_images = set(filename.split('.')[0] for filename in os.listdir(folder_path))\n",
    "\n",
    "# Find the intersection to identify matching images\n",
    "matching_images = csv_ids.intersection(folder_images)\n",
    "\n",
    "# Count the number of matching images\n",
    "num_matching_images = len(matching_images)\n",
    "\n",
    "print(\"Number of matching images:\", num_matching_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Type 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now construct the train \"type 2\" part, thus the 85k pristine with 6_way_label == 1,2,3,5 (4 manipulated deleted) and 85k generated still with 6_way_label == 1,2,3,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44390\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('C:/Users/nello/Documents/vscode_projects/Thesis/3_image_generation/generated_images/csv/train_synthetics.csv')\n",
    "\n",
    "# Filter rows where '6_way_label' equals 1, 2, 3, or 5 and 'fake_id' starts with 'SD', 'DL', or 'GL'\n",
    "filtered_rows = df[(df['6_way_label'].isin([1, 2, 3, 5])) & (df['fake_id'].str.startswith(('SD', 'DL', 'GL')))]\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "print(len(filtered_rows))\n",
    "filtered_rows.to_csv('csv/train/train_class_1_2_3_5_already_generated.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save a csv for the 85k pristine and the 85k-44k to be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train_class_1_2_3_5_already_generated: 44390\n",
      "len of train_class_1_2_3_5_pristine: 85387\n",
      "len of train_class_1_2_3_5_to_be_generated: 40997\n",
      "train_class_1_2_3_5_already_generated: 44.390 + train_class_1_2_3_5_to_be_generated: 40.997 = 85387\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "MULTIMODAL_TRAIN_CLEANED_WITH_CLASS_TSV = \"C:/Users/nello/OneDrive - University of Pisa/TESI/TSV_JSON/1_dataset_cleaning/tsv/train_tsv_with_class.tsv\"\n",
    "ALREADY_GENERATED_FROM_CLASS_1_2_3_5 = \"csv/train/train_class_1_2_3_5_already_generated.csv\"\n",
    "\n",
    "train = pd.read_csv(MULTIMODAL_TRAIN_CLEANED_WITH_CLASS_TSV, sep='/t')\n",
    "train_filtered = train[train['6_way_label'].isin([1, 2, 3, 5])]\n",
    "\n",
    "train_class_0_already_generated = pd.read_csv(ALREADY_GENERATED_FROM_CLASS_1_2_3_5)\n",
    "print(\"len of train_class_1_2_3_5_already_generated: \"+str(len(train_class_0_already_generated)))\n",
    "\n",
    "# Filter rows from train DataFrame that are not in the 'id' column of the other DataFrame\n",
    "train_not_in_other = train_filtered[~train_filtered['id'].isin(train_class_0_already_generated['id'])]\n",
    "\n",
    "# Select the first 107,549 rows from train_not_in_other\n",
    "train_pristine = train_not_in_other.head(85387)\n",
    "\n",
    "# Save train_pristine to CSV\n",
    "print(\"len of train_class_1_2_3_5_pristine: \"+str(len(train_pristine)))\n",
    "train_pristine.to_csv('csv/train/train_class_1_2_3_5_pristine.csv', index=False)\n",
    "\n",
    "# Filter rows from train_filtered that are not in train_class_0_already_generated or train_pristine\n",
    "train_remaining = train_filtered[~train_filtered['id'].isin(train_class_0_already_generated['id']) & ~train_filtered['id'].isin(train_pristine['id'])]\n",
    "\n",
    "# Save the remaining rows to a new CSV file\n",
    "print(\"len of train_class_1_2_3_5_to_be_generated: \"+str(len(train_remaining)))\n",
    "train_remaining.to_csv('csv/train/train_class_1_2_3_5_to_be_generated.csv', index=False)\n",
    "print(\"train_class_1_2_3_5_already_generated: 44.390 + train_class_1_2_3_5_to_be_generated: 40.997 = \" + str(44390+40997))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rename the columns real_image and fake_image to pristine_image and generated_image inside train_class_1_2_3_5_already_generated.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/train/train_class_1_2_3_5_already_generated.csv')\n",
    "\n",
    "# Rename the column\n",
    "df.rename(columns={\"real_image\": \"pristine_image\"}, inplace=True)\n",
    "df.rename(columns={\"fake_image\": \"generated_image\"}, inplace=True)\n",
    "df.to_csv(\"csv/train/train_class_1_2_3_5_already_generated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also change the value of columns real_image and fake_image in train_class_1_2_3_5_already_generated.csv from real_image=1 and fake_image=0 to real_image=0 and fake_image=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/train/train_class_1_2_3_5_already_generated.csv')\n",
    "\n",
    "# Swap the values of the two columns\n",
    "df['pristine_image'], df['generated_image'] = df['generated_image'], df['pristine_image']\n",
    "\n",
    "df.to_csv(\"csv/train/train_class_1_2_3_5_already_generated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduce columns of train_class_1_2_3_5_to_be_generated and train_class_1_2_3_5_pristine csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('csv/train/train_class_1_2_3_5_to_be_generated.csv')\n",
    "\n",
    "# Rename the column\n",
    "df.rename(columns={\"clean_title\": \"original_caption\"}, inplace=True)\n",
    "\n",
    "df['pristine_image'] = 1\n",
    "df['generated_image'] = 0\n",
    "df['real_text'] = 0\n",
    "df['fakenews_text'] = 1\n",
    "\n",
    "# Select only the desired columns in the required order\n",
    "desired_columns = ['id', 'author', 'original_caption', 'num_comments', '6_way_label', 'pristine_image', 'generated_image', 'real_text', 'fakenews_text']\n",
    "df = df[desired_columns]\n",
    "\n",
    "df.to_csv(\"csv/train/train_class_1_2_3_5_to_be_generated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('csv/train/train_class_1_2_3_5_pristine.csv')\n",
    "\n",
    "# Rename the column\n",
    "df.rename(columns={\"clean_title\": \"original_caption\"}, inplace=True)\n",
    "\n",
    "df['pristine_image'] = 1\n",
    "df['generated_image'] = 0\n",
    "df['real_text'] = 0\n",
    "df['fakenews_text'] = 1\n",
    "\n",
    "# Select only the desired columns in the required order\n",
    "desired_columns = ['id', 'author', 'original_caption', 'num_comments', '6_way_label', 'pristine_image', 'generated_image', 'real_text', 'fakenews_text']\n",
    "df = df[desired_columns]\n",
    "\n",
    "df.to_csv(\"csv/train/train_class_1_2_3_5_pristine.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the distribution of the original classes (1,2,3,5) for the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for each unique value of '6_way_label':\n",
      "6_way_label\n",
      "2    52898\n",
      "1    16290\n",
      "5    10484\n",
      "3     5715\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('csv/train/train_class_1_2_3_5_pristine.csv')\n",
    "\n",
    "# Count the number of rows for each value of '6_way_label'\n",
    "label_counts = df['6_way_label'].value_counts()\n",
    "\n",
    "# Print the counts for each unique value of '6_way_label'\n",
    "print(\"Counts for each unique value of '6_way_label':\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for each unique value of '6_way_label':\n",
      "6_way_label\n",
      "2    27339\n",
      "1     8566\n",
      "5     5565\n",
      "3     2920\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('csv/train/train_class_1_2_3_5_already_generated.csv')\n",
    "\n",
    "# Count the number of rows for each value of '6_way_label'\n",
    "label_counts = df['6_way_label'].value_counts()\n",
    "\n",
    "# Print the counts for each unique value of '6_way_label'\n",
    "print(\"Counts for each unique value of '6_way_label':\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for '6_way_label' 1:\n",
      "Starts with 'SD': 3423\n",
      "Starts with 'GL': 1681\n",
      "Starts with 'DL': 3462\n",
      "\n",
      "Counts for '6_way_label' 2:\n",
      "Starts with 'SD': 10887\n",
      "Starts with 'GL': 5429\n",
      "Starts with 'DL': 11023\n",
      "\n",
      "Counts for '6_way_label' 3:\n",
      "Starts with 'SD': 1125\n",
      "Starts with 'GL': 588\n",
      "Starts with 'DL': 1207\n",
      "\n",
      "Counts for '6_way_label' 5:\n",
      "Starts with 'SD': 2215\n",
      "Starts with 'GL': 1086\n",
      "Starts with 'DL': 2264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('csv/train/train_class_1_2_3_5_already_generated.csv')\n",
    "\n",
    "# Group the DataFrame by '6_way_label'\n",
    "grouped = df.groupby('6_way_label')\n",
    "\n",
    "# Iterate over each group\n",
    "for label, group in grouped:\n",
    "    # Filter the group where 'id' column starts with \"SD\", \"GL\", or \"DL\"\n",
    "    sd_count = len(group[group['fake_id'].str.startswith('SD')])\n",
    "    gl_count = len(group[group['fake_id'].str.startswith('GL')])\n",
    "    dl_count = len(group[group['fake_id'].str.startswith('DL')])\n",
    "    \n",
    "    # Print counts for each value of '6_way_label'\n",
    "    print(f\"Counts for '6_way_label' {label}:\")\n",
    "    print(f\"Starts with 'SD': {sd_count}\")\n",
    "    print(f\"Starts with 'GL': {gl_count}\")\n",
    "    print(f\"Starts with 'DL': {dl_count}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated with SD:\n",
      "6_way_label: 1 in the interval [0, 12350): 2356\n",
      "6_way_label: 2 in the interval [0, 12350): 7614\n",
      "6_way_label: 3 in the interval [0, 12350): 860\n",
      "6_way_label: 5 in the interval [0, 12350): 1520\n",
      "\n",
      "Generated with DL:\n",
      "6_way_label: 1 in the interval [12350, 24394): 2315\n",
      "6_way_label: 2 in the interval [12350, 24394): 7437\n",
      "6_way_label: 3 in the interval [12350, 24394): 806\n",
      "6_way_label: 5 in the interval [12350, 24394): 1486\n",
      "\n",
      "Generated with GL:\n",
      "6_way_label: 1 in the interval [24394, 40997): 3146\n",
      "6_way_label: 2 in the interval [24394, 40997): 10315\n",
      "6_way_label: 3 in the interval [24394, 40997): 1080\n",
      "6_way_label: 5 in the interval [24394, 40997): 2062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('csv/train/train_class_1_2_3_5_to_be_generated.csv')\n",
    "\n",
    "# Define the start and end indices for each interval\n",
    "intervals = {\n",
    "    \"Generated with SD\": (0, 12350),\n",
    "    \"Generated with DL\": (12350, 24394),\n",
    "    \"Generated with GL\": (24394, len(df))\n",
    "}\n",
    "\n",
    "# Iterate over each interval\n",
    "for method, (start_index, end_index) in intervals.items():\n",
    "    # Filter the DataFrame based on the interval\n",
    "    interval_df = df.iloc[start_index:end_index]\n",
    "    \n",
    "    # Group the interval DataFrame by '6_way_label'\n",
    "    grouped = interval_df.groupby('6_way_label')\n",
    "    \n",
    "    # Print the count of rows with each '6_way_label' value within the interval\n",
    "    print(f\"{method}:\")\n",
    "    for label, group in grouped:\n",
    "        label_count = len(group)\n",
    "        print(f\"6_way_label: {label} in the interval [{start_index}, {end_index}): {label_count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if every id inside the train_tsv_with_class.csv with '6_way_label' = 1,2,3,5 is contained in the union of the 3 csv train_class_1_2_3_5_pristine + train_class_1_2_3_5_already_generated + train_class_1_2_3_5_to_be_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170774\n",
      "170774\n",
      "The union of IDs from the first three CSVs corresponds exactly to the IDs from train_tsv_with_class.csv where '6_way_label' = 1,2,3,5.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the 'id' column from the first three CSV files into sets\n",
    "csv1_ids = set(pd.read_csv(\"csv/train/train_class_1_2_3_5_pristine.csv\")['id'])\n",
    "csv2_ids = set(pd.read_csv(\"csv/train/train_class_1_2_3_5_to_be_generated.csv\")['id'])\n",
    "csv3_ids = set(pd.read_csv(\"csv/train/train_class_1_2_3_5_already_generated.csv\")['id'])\n",
    "\n",
    "# Find the union of these three sets\n",
    "union_ids = csv1_ids.union(csv2_ids, csv3_ids)\n",
    "\n",
    "csv4_ids = set(pd.read_csv(\"C:/Users/nello/OneDrive - University of Pisa/TESI/TSV_JSON/1_dataset_cleaning/tsv/train_tsv_with_class.tsv\", sep='\\t').loc[pd.read_csv(\"C:/Users/nello/OneDrive - University of Pisa/TESI/TSV_JSON/1_dataset_cleaning/tsv/train_tsv_with_class.tsv\", sep='\\t')['6_way_label'].isin([1, 2, 3, 5]), 'id'])\n",
    "\n",
    "print(len(union_ids))\n",
    "print(len(csv4_ids))\n",
    "\n",
    "# Check if the union_ids set is equal to the csv4_ids set\n",
    "if union_ids == csv4_ids:\n",
    "    print(\"The union of IDs from the first three CSVs corresponds exactly to the IDs from train_tsv_with_class.csv where '6_way_label' = 1,2,3,5.\")\n",
    "else:\n",
    "    print(\"The union of IDs from the first three CSVs does not correspond exactly to the IDs from train_tsv_with_class.csv where '6_way_label' = 1,2,3,5.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, to get to 85.387 generated images i want to generate other 40.997:\n",
    "\n",
    "- SD: 30.000 - 17.650 = 12.350\n",
    "- DL: 30.000 - 17.956 = 12.044\n",
    "- GL: (85.387 - 60.000) - 8.784 = 16.603\n",
    "\n",
    "so 40.997 in total from the train_class_1_2_3_5_to_be_generated.csv, but first i need to caption all the images from train_class_1_2_3_5_to_be_generated.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that every pristine image inside train_class_1_2_3_5_to_be_generated csv is already in the image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching images: 40997\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the 'id' column from the CSV file into a set\n",
    "csv_ids = set(pd.read_csv(\"csv/train/train_class_1_2_3_5_to_be_generated.csv\")['id'])\n",
    "\n",
    "# Get a list of all the image filenames in the folder\n",
    "folder_path = \"C:/Users/nello/Desktop/TESI/dataset_after_merging_WITH_DUPLICATES\"\n",
    "folder_images = set(filename.split('.')[0] for filename in os.listdir(folder_path))\n",
    "\n",
    "# Find the intersection to identify matching images\n",
    "matching_images = csv_ids.intersection(folder_images)\n",
    "\n",
    "# Count the number of matching images\n",
    "num_matching_images = len(matching_images)\n",
    "\n",
    "print(\"Number of matching images:\", num_matching_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that every pristine image inside train_class_1_2_3_5_pristine csv is already in the image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching images: 85387\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the 'id' column from the CSV file into a set\n",
    "csv_ids = set(pd.read_csv(\"csv/train/train_class_1_2_3_5_pristine.csv\")['id'])\n",
    "\n",
    "# Get a list of all the image filenames in the folder\n",
    "folder_path = \"/home/enriconello/DeepFakeDetection/dataset\"\n",
    "folder_images = set(filename.split('.')[0] for filename in os.listdir(folder_path))\n",
    "\n",
    "# Find the intersection to identify matching images\n",
    "matching_images = csv_ids.intersection(folder_images)\n",
    "\n",
    "# Count the number of matching images\n",
    "num_matching_images = len(matching_images)\n",
    "\n",
    "print(\"Number of matching images:\", num_matching_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that every pristine image inside train_class_1_2_3_5_already_generated csv is already in the image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching images: 44390\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the 'id' column from the CSV file into a set\n",
    "csv_ids = set(pd.read_csv(\"csv/train/train_class_1_2_3_5_already_generated.csv\")['fake_id'])\n",
    "\n",
    "# Get a list of all the image filenames in the folder\n",
    "folder_path = \"/home/enriconello/DeepFakeDetection/dataset\"\n",
    "folder_images = set(filename.split('.')[0] for filename in os.listdir(folder_path))\n",
    "\n",
    "# Find the intersection to identify matching images\n",
    "matching_images = csv_ids.intersection(folder_images)\n",
    "\n",
    "# Count the number of matching images\n",
    "num_matching_images = len(matching_images)\n",
    "\n",
    "print(\"Number of matching images:\", num_matching_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the dataset folder from Manipulated Images (class 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images removed: 190185\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "csv_files = ['/home/enriconello/DeepFakeDetection/Thesis/3_image_generation/generated_images/csv/train.csv',\n",
    "             '/home/enriconello/DeepFakeDetection/Thesis/3_image_generation/generated_images/csv/test_with_duplicates.csv',\n",
    "             '/home/enriconello/DeepFakeDetection/Thesis/3_image_generation/generated_images/csv/validation_with_duplicates.csv']\n",
    "\n",
    "# image folder path \n",
    "image_folder = '/home/enriconello/DeepFakeDetection/dataset'\n",
    "\n",
    "unique_ids = set()\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Filter rows where '6_way_label' equals to the class Manipulated images\n",
    "    filtered_df = df[df['6_way_label'] == 4]\n",
    "    unique_ids.update(filtered_df['id'])\n",
    "\n",
    "num_images_removed = 0\n",
    "\n",
    "for id_value in unique_ids:\n",
    "    image_path = os.path.join(image_folder, f\"{id_value}.jpg\")\n",
    "    \n",
    "    if os.path.exists(image_path):\n",
    "        os.remove(image_path)\n",
    "        num_images_removed += 1\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "print(f\"Total number of images removed: {num_images_removed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the train_class_1_2_3_5_captioned csv, deleting the generated captions that starts with \"araf\" (buggy captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of updated rows: 14525\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "original_df = pd.read_csv('/home/enriconello/DeepFakeDetection/Thesis/5_biased_detection/csv/train/train_class_1_2_3_5_captioned.csv')\n",
    "\n",
    "# Create a copy of the original DataFrame\n",
    "modified_df = original_df.copy()\n",
    "\n",
    "# Remove words starting with \"araf\" in the \"generated_caption\" column\n",
    "modified_df['generated_caption'] = modified_df['generated_caption'].str.replace(r'\\baraf\\w*\\b', '', regex=True)\n",
    "\n",
    "# Remove leading and trailing whitespace from the \"generated_caption\" column\n",
    "modified_df['generated_caption'] = modified_df['generated_caption'].str.strip()\n",
    "\n",
    "# Save the modified DataFrame to the same CSV file\n",
    "modified_df.to_csv(\"/home/enriconello/DeepFakeDetection/Thesis/5_biased_detection/csv/train/train_class_1_2_3_5_captioned.csv\", index=False)\n",
    "\n",
    "# Count the number of updated rows\n",
    "updated_rows_count = (original_df['generated_caption'] != modified_df['generated_caption']).sum()\n",
    "\n",
    "print(f\"Number of updated rows: {updated_rows_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After fake generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating all the fakes images that were missing, i need to merge the class_0_to_be_generated and the class_0_already_generated, and the same for class 1,2,3,5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting by adding the \"fake_id\" column to the train_class_0_to_be_generated csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/enriconello/DeepFakeDetection/Thesis/5_biased_detection/csv/train/train_class_0_to_be_generated.csv\")\n",
    "\n",
    "# new column 'fake_id'\n",
    "df['fake_id'] = ''\n",
    "\n",
    "# assign values to 'fake_id' based on row index intervals\n",
    "df.loc[df.index < 17650, 'fake_id'] = 'SD_fake_' + df['id']\n",
    "df.loc[(df.index >= 17650) & (df.index < 35606), 'fake_id'] = 'DL_fake_' + df['id']\n",
    "df.loc[df.index >= 35606, 'fake_id'] = 'GL_fake_' + df['id']\n",
    "\n",
    "# 'fake_id' column next to 'id' column\n",
    "df.insert(df.columns.get_loc('id') + 1, 'fake_id', df.pop('fake_id'))\n",
    "\n",
    "df.to_csv('/home/enriconello/DeepFakeDetection/Thesis/5_biased_detection/csv/train/train_class_0_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/enriconello/DeepFakeDetection/Thesis/5_biased_detection/csv/train/train_class_1_2_3_5_to_be_generated.csv\")\n",
    "\n",
    "# new column 'fake_id'\n",
    "df['fake_id'] = ''\n",
    "\n",
    "# assign values to 'fake_id' based on row index intervals\n",
    "df.loc[df.index < 12350, 'fake_id'] = 'SD_fake_' + df['id']\n",
    "df.loc[(df.index >= 12350) & (df.index < 24394), 'fake_id'] = 'DL_fake_' + df['id']\n",
    "df.loc[df.index >= 24394, 'fake_id'] = 'GL_fake_' + df['id']\n",
    "\n",
    "# 'fake_id' column next to 'id' column\n",
    "df.insert(df.columns.get_loc('id') + 1, 'fake_id', df.pop('fake_id'))\n",
    "\n",
    "df.to_csv('/home/enriconello/DeepFakeDetection/Thesis/5_biased_detection/csv/train/train_class_1_2_3_5_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set the column pristine_image = 0 and generated_image = 1 for every row in train_class_0_final csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/train/train_class_0_final.csv')\n",
    "\n",
    "# Swap the values of the two columns\n",
    "df['pristine_image'], df['generated_image'] = df['generated_image'], df['pristine_image']\n",
    "\n",
    "df.to_csv(\"csv/train/train_class_0_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/train/train_class_1_2_3_5_final.csv')\n",
    "\n",
    "# Swap the values of the two columns\n",
    "df['pristine_image'], df['generated_image'] = df['generated_image'], df['pristine_image']\n",
    "\n",
    "df.to_csv(\"csv/train/train_class_1_2_3_5_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now i need to add the column \"generated_caption\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fake_id</th>\n",
       "      <th>author</th>\n",
       "      <th>original_caption</th>\n",
       "      <th>generated_caption</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>6_way_label</th>\n",
       "      <th>pristine_image</th>\n",
       "      <th>generated_image</th>\n",
       "      <th>real_text</th>\n",
       "      <th>fakenews_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dhr269</td>\n",
       "      <td>SD_fake_dhr269</td>\n",
       "      <td>bico89</td>\n",
       "      <td>my bill was</td>\n",
       "      <td>someone is holding a receipt and a box of cheese</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3gb4sc</td>\n",
       "      <td>SD_fake_3gb4sc</td>\n",
       "      <td>landofthemolotovia</td>\n",
       "      <td>quebec considering removing nword from place n...</td>\n",
       "      <td>there is a bear that is walking across the water</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1wkhcb</td>\n",
       "      <td>SD_fake_1wkhcb</td>\n",
       "      <td>Zjw0115</td>\n",
       "      <td>dramatic baseball catcher</td>\n",
       "      <td>baseball player catching a ball in a stadium</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bt0p1w</td>\n",
       "      <td>SD_fake_bt0p1w</td>\n",
       "      <td>tx69er</td>\n",
       "      <td>this mcdonalds uses aol for its corporate email</td>\n",
       "      <td>a close up of a receipt with a price tag on it</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2yfbtu</td>\n",
       "      <td>SD_fake_2yfbtu</td>\n",
       "      <td>ProfessorFartdust</td>\n",
       "      <td>fan and katy perry</td>\n",
       "      <td>woman with a pink and blue hair and a man with...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id         fake_id              author  \\\n",
       "0  dhr269  SD_fake_dhr269              bico89   \n",
       "1  3gb4sc  SD_fake_3gb4sc  landofthemolotovia   \n",
       "2  1wkhcb  SD_fake_1wkhcb             Zjw0115   \n",
       "3  bt0p1w  SD_fake_bt0p1w              tx69er   \n",
       "4  2yfbtu  SD_fake_2yfbtu   ProfessorFartdust   \n",
       "\n",
       "                                    original_caption  \\\n",
       "0                                        my bill was   \n",
       "1  quebec considering removing nword from place n...   \n",
       "2                          dramatic baseball catcher   \n",
       "3    this mcdonalds uses aol for its corporate email   \n",
       "4                                 fan and katy perry   \n",
       "\n",
       "                                   generated_caption  num_comments  \\\n",
       "0   someone is holding a receipt and a box of cheese          10.0   \n",
       "1   there is a bear that is walking across the water          19.0   \n",
       "2       baseball player catching a ball in a stadium           6.0   \n",
       "3     a close up of a receipt with a price tag on it           3.0   \n",
       "4  woman with a pink and blue hair and a man with...           5.0   \n",
       "\n",
       "   6_way_label  pristine_image  generated_image  real_text  fakenews_text  \n",
       "0            0               0                1          1              0  \n",
       "1            0               0                1          1              0  \n",
       "2            0               0                1          1              0  \n",
       "3            0               0                1          1              0  \n",
       "4            0               0                1          1              0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_a = pd.read_csv(\"csv/train/train_class_0_final.csv\")\n",
    "df_b = pd.read_csv(\"csv/train/train_class_0_captioned.csv\")\n",
    "\n",
    "# Perform inner join on 'id' column\n",
    "df_c = pd.merge(df_a, df_b, on='id', how='inner')\n",
    "\n",
    "# Rename 'original_caption_x' to 'original_caption'\n",
    "df_c = df_c.rename(columns={'original_caption_x': 'original_caption'})\n",
    "\n",
    "df_c = df_c[['id', 'fake_id', 'author', 'original_caption', 'generated_caption', 'num_comments', '6_way_label', 'pristine_image', 'generated_image', 'real_text', 'fakenews_text']]\n",
    "\n",
    "df_c.to_csv(\"csv/train/train_class_0_final.csv\", index=False)\n",
    "df_c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fake_id</th>\n",
       "      <th>author</th>\n",
       "      <th>original_caption</th>\n",
       "      <th>generated_caption</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>6_way_label</th>\n",
       "      <th>pristine_image</th>\n",
       "      <th>generated_image</th>\n",
       "      <th>real_text</th>\n",
       "      <th>fakenews_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110klp</td>\n",
       "      <td>SD_fake_110klp</td>\n",
       "      <td>kopiikat</td>\n",
       "      <td>look at all the people having fun down there</td>\n",
       "      <td>crowd of people at a concert with their hands ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8jthnp</td>\n",
       "      <td>SD_fake_8jthnp</td>\n",
       "      <td>Yare_Daze</td>\n",
       "      <td>zenith hereby unseen higher places</td>\n",
       "      <td>there is a poster of a man riding a motorcycle...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77x9zc</td>\n",
       "      <td>SD_fake_77x9zc</td>\n",
       "      <td>NikKerk</td>\n",
       "      <td>rare footage of a british soldier using a flam...</td>\n",
       "      <td>there is a black and white photo of a person i...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67yaae</td>\n",
       "      <td>SD_fake_67yaae</td>\n",
       "      <td>shittyusernamee</td>\n",
       "      <td>years ago i was burning an sat book for vengea...</td>\n",
       "      <td>book with a burning skull on it sitting on the...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95hg84</td>\n",
       "      <td>SD_fake_95hg84</td>\n",
       "      <td>politics_SS</td>\n",
       "      <td>transgender troops say trump has taken few ste...</td>\n",
       "      <td>a close up of a brick wall with a pink tongue ...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id         fake_id           author  \\\n",
       "0  110klp  SD_fake_110klp         kopiikat   \n",
       "1  8jthnp  SD_fake_8jthnp        Yare_Daze   \n",
       "2  77x9zc  SD_fake_77x9zc          NikKerk   \n",
       "3  67yaae  SD_fake_67yaae  shittyusernamee   \n",
       "4  95hg84  SD_fake_95hg84      politics_SS   \n",
       "\n",
       "                                    original_caption  \\\n",
       "0       look at all the people having fun down there   \n",
       "1                 zenith hereby unseen higher places   \n",
       "2  rare footage of a british soldier using a flam...   \n",
       "3  years ago i was burning an sat book for vengea...   \n",
       "4  transgender troops say trump has taken few ste...   \n",
       "\n",
       "                                   generated_caption  num_comments  \\\n",
       "0  crowd of people at a concert with their hands ...           0.0   \n",
       "1  there is a poster of a man riding a motorcycle...           0.0   \n",
       "2  there is a black and white photo of a person i...           1.0   \n",
       "3  book with a burning skull on it sitting on the...           5.0   \n",
       "4  a close up of a brick wall with a pink tongue ...          20.0   \n",
       "\n",
       "   6_way_label  pristine_image  generated_image  real_text  fakenews_text  \n",
       "0            2               0                1          0              1  \n",
       "1            1               0                1          0              1  \n",
       "2            2               0                1          0              1  \n",
       "3            2               0                1          0              1  \n",
       "4            3               0                1          0              1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_a = pd.read_csv(\"csv/train/train_class_1_2_3_5_final.csv\")\n",
    "df_b = pd.read_csv(\"csv/train/train_class_1_2_3_5_captioned.csv\")\n",
    "\n",
    "# Perform inner join on 'id' column\n",
    "df_c = pd.merge(df_a, df_b, on='id', how='inner')\n",
    "\n",
    "# Rename 'original_caption_x' to 'original_caption'\n",
    "df_c = df_c.rename(columns={'original_caption_x': 'original_caption'})\n",
    "\n",
    "df_c = df_c[['id', 'fake_id', 'author', 'original_caption', 'generated_caption', 'num_comments', '6_way_label', 'pristine_image', 'generated_image', 'real_text', 'fakenews_text']]\n",
    "\n",
    "df_c.to_csv(\"csv/train/train_class_1_2_3_5_final.csv\", index=False)\n",
    "df_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally merge train_class_0_final + train_class_0_already_generated + train_class_1_2_3_5_final + train_class_1_2_3_5_already_generated to create train_final csv containing all the 192.935 generated images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "firstly drop the column class from train_class_0_already_generated and train_class_1_2_3_5_already_generated to make all the csv equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/enriconello/DeepFakeDetection/Thesis/5_biased_detection/csv/train/train_class_1_2_3_5_already_generated.csv\")\n",
    "\n",
    "# Drop the \"class\" column\n",
    "column_to_drop = 'class'\n",
    "df = df.drop(columns=[column_to_drop])\n",
    "\n",
    "# Save the modified DataFrame back to a new CSV file\n",
    "df.to_csv(\"/home/enriconello/DeepFakeDetection/Thesis/5_biased_detection/csv/train/train_class_1_2_3_5_already_generated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and finally make the union of the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192936\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read CSV files\n",
    "df1 = pd.read_csv(\"csv/train/train_class_0_final.csv\")\n",
    "df3 = pd.read_csv(\"csv/train/train_class_0_already_generated.csv\")\n",
    "df2 = pd.read_csv(\"csv/train/train_class_1_2_3_5_final.csv\")\n",
    "df4 = pd.read_csv(\"csv/train/train_class_1_2_3_5_already_generated.csv\")\n",
    "\n",
    "# Concatenate DataFrames\n",
    "df_union = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "\n",
    "df_union.to_csv(\"csv/train/train_generated_final.csv\", index=False)\n",
    "print(len(df_union))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the cardinalities of the classes with the generation methods associated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for '6_way_label' 0:\n",
      "Starts with 'SD': 40000\n",
      "Starts with 'GL': 27549\n",
      "Starts with 'DL': 40000\n",
      "\n",
      "Counts for '6_way_label' 1:\n",
      "Starts with 'SD': 5779\n",
      "Starts with 'GL': 4827\n",
      "Starts with 'DL': 5777\n",
      "\n",
      "Counts for '6_way_label' 2:\n",
      "Starts with 'SD': 18501\n",
      "Starts with 'GL': 15744\n",
      "Starts with 'DL': 18460\n",
      "\n",
      "Counts for '6_way_label' 3:\n",
      "Starts with 'SD': 1985\n",
      "Starts with 'GL': 1668\n",
      "Starts with 'DL': 2013\n",
      "\n",
      "Counts for '6_way_label' 5:\n",
      "Starts with 'SD': 3735\n",
      "Starts with 'GL': 3148\n",
      "Starts with 'DL': 3750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('csv/train/train_generated_final.csv')\n",
    "\n",
    "# Group the DataFrame by '6_way_label'\n",
    "grouped = df.groupby('6_way_label')\n",
    "\n",
    "# Iterate over each group\n",
    "for label, group in grouped:\n",
    "    # Filter the group where 'id' column starts with \"SD\", \"GL\", or \"DL\"\n",
    "    sd_count = len(group[group['fake_id'].str.startswith('SD')])\n",
    "    gl_count = len(group[group['fake_id'].str.startswith('GL')])\n",
    "    dl_count = len(group[group['fake_id'].str.startswith('DL')])\n",
    "    \n",
    "    # Print counts for each value of '6_way_label'\n",
    "    print(f\"Counts for '6_way_label' {label}:\")\n",
    "    print(f\"Starts with 'SD': {sd_count}\")\n",
    "    print(f\"Starts with 'GL': {gl_count}\")\n",
    "    print(f\"Starts with 'DL': {dl_count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now i need to merge validation_class_0_pristine and train_class_1_2_3_5_pristine to make train_pristine_final csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192935\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"csv/train/train_class_0_pristine.csv\")\n",
    "df2 = pd.read_csv(\"csv/train/train_class_1_2_3_5_pristine.csv\")\n",
    "\n",
    "# Concatenate DataFrames\n",
    "df_union = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "df_union.to_csv(\"csv/train/train_pristine_final.csv\", index=False)\n",
    "print(len(df_union))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally i need to merge the train_pristine_final with train_generated_final to generated train csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385871\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_a = pd.read_csv('csv/train/train_generated_final.csv')\n",
    "df_b = pd.read_csv('csv/train/train_pristine_final.csv')\n",
    "\n",
    "# Add missing columns from A to B with empty values\n",
    "for column in df_a.columns:\n",
    "    if column not in df_b.columns:\n",
    "        df_b[column] = ''\n",
    "\n",
    "# Concatenate dataframes\n",
    "df_c = pd.concat([df_a, df_b])\n",
    "\n",
    "print(len(df_c))\n",
    "df_c.to_csv('csv/train/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that half of them is generated, so the other half is pristine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1808720/1061100489.py:3: DtypeWarning: Columns (1,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('csv/train/train.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where fake_id starts with 'SD', 'DL', or 'GL', or is empty: 192936\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('csv/train/train.csv')\n",
    "\n",
    "# Filter rows where the \"fake_id\" column starts with \"SD\", \"DL\", or \"GL\", or is empty\n",
    "filtered_df = df[df['fake_id'].apply(lambda x: str(x).startswith(('SD', 'DL', 'GL')))]\n",
    "\n",
    "print(\"Number of rows where fake_id starts with 'SD', 'DL', or 'GL', or is empty:\", len(filtered_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching images: 385871\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"csv/train/train.csv\")\n",
    "\n",
    "selected_ids = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['pristine_image'] == 1:\n",
    "        selected_ids.append(row['id'])\n",
    "    else:\n",
    "        selected_ids.append(row['fake_id'])\n",
    "\n",
    "# Get a list of all the image filenames in the folder (without extensions)\n",
    "folder_path = \"/home/enriconello/DeepFakeDetection/dataset_new/train\"\n",
    "folder_images = set(os.path.splitext(filename)[0] for filename in os.listdir(folder_path))\n",
    "\n",
    "# Find the intersection to identify matching images\n",
    "matching_images = set(selected_ids).intersection(folder_images)\n",
    "num_matching_images = len(matching_images)\n",
    "\n",
    "print(\"Number of matching images:\", num_matching_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set column \"class\" = 0 if column \"fake_id\" is not empty, = 1 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1663797/943676688.py:3: DtypeWarning: Columns (1,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"csv/train/train.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"csv/train/train.csv\")\n",
    "\n",
    "df['class'] = df['fake_id'].apply(lambda x: 0 if pd.isnull(x) else 1)\n",
    "\n",
    "df.to_csv(\"csv/train/train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shuffle the csv row order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1663797/1934373135.py:3: DtypeWarning: Columns (1,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('/home/enriconello/DeepFakeDetection/Thesis/5_biased_detection/csv/train/train.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/home/enriconello/DeepFakeDetection/Thesis/5_biased_detection/csv/train/train.csv')\n",
    "\n",
    "# Shuffle the rows of the DataFrame\n",
    "df_shuffled = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df_shuffled.to_csv('/home/enriconello/DeepFakeDetection/Thesis/5_biased_detection/csv/train/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Train Biased Type 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruct the csv for train, deleting all the rows of generated samples from class 0 (type 1). Then it will contain only pristine class 0, pristine + generated class 1,2,3,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len before: 385871\n",
      "len after: 278322\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/nello/Documents/vscode_projects/Thesis/5_biased_detection/csv/train/train.csv\")\n",
    "print(\"len before: \"+str(len(df)))\n",
    "\n",
    "filtered_df = df[~((df['generated_image'] == 1) & (df['real_text'] == 1))]\n",
    "\n",
    "filtered_df.to_csv(\"C:/Users/nello/Documents/vscode_projects/Thesis/5_biased_detection/csv/train/train_biased_2.csv\", index=False)\n",
    "print(\"len after: \"+str(len(filtered_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a column \"type\" that will be = 1 if the 6_way_label == 0, else will be type = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('C:/Users/nello/Documents/vscode_projects/Thesis/5_biased_detection/csv/train/train_biased_2.csv')\n",
    "\n",
    "df['type'] = df['6_way_label'].apply(lambda x: 1 if x == 0 else 2)\n",
    "\n",
    "df.to_csv('C:/Users/nello/Documents/vscode_projects/Thesis/5_biased_detection/csv/train/train_biased_2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
