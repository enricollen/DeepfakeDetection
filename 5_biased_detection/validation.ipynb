{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Type 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VALIDATION CLASS 0: collect the fake images already generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5602\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('C:/Users/nello/Documents/vscode_projects/Thesis/3_image_generation/generated_images/csv/validation_synthetics.csv')\n",
    "\n",
    "# Filter rows where '6_way_label' equals 0 and 'fake_id' starts with 'SD', 'DL', or 'GL'\n",
    "filtered_rows_SD = df[(df['6_way_label'] == 0) & (df['fake_id'].str.startswith('SD'))]\n",
    "filtered_rows_DL = df[(df['6_way_label'] == 0) & (df['fake_id'].str.startswith('DL'))]\n",
    "filtered_rows_GL = df[(df['6_way_label'] == 0) & (df['fake_id'].str.startswith('GL'))]\n",
    "\n",
    "# Concatenate columns for all groups\n",
    "result = pd.concat([filtered_rows_SD, filtered_rows_DL, filtered_rows_GL])\n",
    "print(len(result))\n",
    "\n",
    "result.to_csv('C:/Users/nello/Documents/vscode_projects/Thesis/5_biased_detection/csv/validation/validation_class_0_already_generated.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save a csv for the 11.490 pristine and the 11.490 - 5.602 to be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of validation_class_0_already_generated: 5602\n",
      "len of validation_class_0_pristine: 11491\n",
      "len of validation_class_0_to_be_generated: 5888\n",
      "validation_class_0_already_generated: 5602 + validation_class_0_to_be_generated: 5888 = 11490\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "MULTIMODAL_VAL_CLEANED_WITH_CLASS_TSV = \"C:/Users/nello/OneDrive - University of Pisa/TESI/TSV_JSON/1_dataset_cleaning/tsv/val_tsv_with_class.tsv\"\n",
    "ALREADY_GENERATED_FROM_CLASS_0 = \"csv/validation/validation_class_0_already_generated.csv\"\n",
    "\n",
    "validation = pd.read_csv(MULTIMODAL_VAL_CLEANED_WITH_CLASS_TSV, sep='\\t')\n",
    "validation_filtered = validation[validation['6_way_label'] == 0]\n",
    "\n",
    "validation_class_0_already_generated = pd.read_csv(ALREADY_GENERATED_FROM_CLASS_0)\n",
    "print(\"len of validation_class_0_already_generated: \"+str(len(validation_class_0_already_generated)))\n",
    "\n",
    "# Filter rows from train DataFrame that are not in the 'id' column of the other DataFrame\n",
    "validation_not_in_other = validation_filtered[~validation_filtered['id'].isin(validation_class_0_already_generated['id'])]\n",
    "\n",
    "# Select the first 11,490 rows from train_not_in_other\n",
    "validation_pristine = validation_not_in_other.head(11491)\n",
    "\n",
    "# Save train_pristine to CSV\n",
    "print(\"len of validation_class_0_pristine: \"+str(len(validation_pristine)))\n",
    "validation_pristine.to_csv('csv/validation/validation_class_0_pristine.csv', index=False)\n",
    "\n",
    "# Filter rows from train_filtered that are not in train_class_0_already_generated or train_pristine\n",
    "validation_remaining = validation_filtered[~validation_filtered['id'].isin(validation_class_0_already_generated['id']) & ~validation_filtered['id'].isin(validation_pristine['id'])]\n",
    "\n",
    "# Save the remaining rows to a new CSV file\n",
    "print(\"len of validation_class_0_to_be_generated: \"+str(len(validation_remaining)))\n",
    "validation_remaining.to_csv('csv/validation/validation_class_0_to_be_generated.csv', index=False)\n",
    "print(\"validation_class_0_already_generated: \"+str(len(validation_class_0_already_generated))+\" + validation_class_0_to_be_generated: \"+str(len(validation_remaining))+\" = \" + str(len(validation_class_0_already_generated)+len(validation_remaining)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rename the columns real_image and fake_image to pristine_image and generated_image inside validation_class_0_already_generated.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/validation/validation_class_0_already_generated.csv')\n",
    "\n",
    "# Rename the column\n",
    "df.rename(columns={\"real_image\": \"pristine_image\"}, inplace=True)\n",
    "df.rename(columns={\"fake_image\": \"generated_image\"}, inplace=True)\n",
    "df.to_csv(\"csv/validation/validation_class_0_already_generated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also change the value of columns real_image and fake_image in validation_class_0_already_generated.csv from real_image=1 and fake_image=0 to real_image=0 and fake_image=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/validation/validation_class_0_already_generated.csv')\n",
    "\n",
    "# Swap the values of the two columns\n",
    "df['pristine_image'], df['generated_image'] = df['generated_image'], df['pristine_image']\n",
    "\n",
    "df.to_csv(\"csv/validation/validation_class_0_already_generated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduce columns of validation_class_0_to_be_generated and validation_class_0_pristine csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('csv/validation/validation_class_0_to_be_generated.csv')\n",
    "\n",
    "# Rename the column\n",
    "df.rename(columns={\"clean_title\": \"original_caption\"}, inplace=True)\n",
    "\n",
    "df['pristine_image'] = 1\n",
    "df['generated_image'] = 0\n",
    "df['real_text'] = 1\n",
    "df['fakenews_text'] = 0\n",
    "\n",
    "# Select only the desired columns in the required order\n",
    "desired_columns = ['id', 'author', 'original_caption', 'num_comments', '6_way_label', 'pristine_image', 'generated_image', 'real_text', 'fakenews_text']\n",
    "df = df[desired_columns]\n",
    "\n",
    "df.to_csv(\"csv/validation/validation_class_0_to_be_generated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('csv/validation/validation_class_0_pristine.csv')\n",
    "\n",
    "# Rename the column\n",
    "df.rename(columns={\"clean_title\": \"original_caption\"}, inplace=True)\n",
    "\n",
    "df['pristine_image'] = 1\n",
    "df['generated_image'] = 0\n",
    "df['real_text'] = 1\n",
    "df['fakenews_text'] = 0\n",
    "\n",
    "# Select only the desired columns in the required order\n",
    "desired_columns = ['id', 'author', 'original_caption', 'num_comments', '6_way_label', 'pristine_image', 'generated_image', 'real_text', 'fakenews_text']\n",
    "df = df[desired_columns]\n",
    "\n",
    "df.to_csv(\"csv/validation/validation_class_0_pristine.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if every id inside the validation_tsv_with_class.csv is contained in the union of the 3 csv validation_pristine + validation_class_0_already_generated + validation_class_0_to_be_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22981\n",
      "22981\n",
      "The union of IDs from the first three CSVs corresponds exactly to the IDs from validation_tsv_with_class.csv where '6_way_label' = 0.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the 'id' column from the first three CSV files into sets\n",
    "csv1_ids = set(pd.read_csv(\"csv/validation/validation_class_0_pristine.csv\")['id'])\n",
    "csv2_ids = set(pd.read_csv(\"csv/validation/validation_class_0_to_be_generated.csv\")['id'])\n",
    "csv3_ids = set(pd.read_csv(\"csv/validation/validation_class_0_already_generated.csv\")['id'])\n",
    "\n",
    "# Find the union of these three sets\n",
    "union_ids = csv1_ids.union(csv2_ids, csv3_ids)\n",
    "\n",
    "csv4_ids = set(pd.read_csv(\"C:/Users/nello/OneDrive - University of Pisa/TESI/TSV_JSON/1_dataset_cleaning/tsv/val_tsv_with_class.tsv\", sep='\\t').loc[pd.read_csv(\"C:/Users/nello/OneDrive - University of Pisa/TESI/TSV_JSON/1_dataset_cleaning/tsv/val_tsv_with_class.tsv\", sep='\\t')['6_way_label'] == 0, 'id'])\n",
    "\n",
    "print(len(union_ids))\n",
    "print(len(csv4_ids))\n",
    "\n",
    "# Check if the union_ids set is equal to the csv4_ids set\n",
    "if union_ids == csv4_ids:\n",
    "    print(\"The union of IDs from the first three CSVs corresponds exactly to the IDs from validation_tsv_with_class.csv where '6_way_label' = 0.\")\n",
    "else:\n",
    "    print(\"The union of IDs from the first three CSVs does not correspond exactly to the IDs from validation_tsv_with_class.csv where '6_way_label' = 0.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now i print out how many StableDiffusion, Dreamlike and Glide images i have already generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where 'fake_id' starts with 'SD': 2263\n",
      "Number of rows where 'fake_id' starts with 'DL': 2242\n",
      "Number of rows where 'fake_id' starts with 'GL': 1097\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"csv/validation/validation_class_0_already_generated.csv\")\n",
    "\n",
    "# Count the rows where the 'fake_id' column starts with 'SD', 'DL', or 'GL'\n",
    "count_SD = len(df[df['fake_id'].str.startswith('SD')])\n",
    "count_DL = len(df[df['fake_id'].str.startswith('DL')])\n",
    "count_GL = len(df[df['fake_id'].str.startswith('GL')])\n",
    "\n",
    "print(\"Number of rows where 'fake_id' starts with 'SD':\", count_SD)\n",
    "print(\"Number of rows where 'fake_id' starts with 'DL':\", count_DL)\n",
    "print(\"Number of rows where 'fake_id' starts with 'GL':\", count_GL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, to get to 11.490 generated images i want to generate other:\n",
    "\n",
    "- SD: 4.000 - 2.263 = 1.737\n",
    "- DL: 4.000 - 2.242 = 1.758\n",
    "- GL: (11.490 - 8.000) - 1.097 = 2.393\n",
    "\n",
    "so 5.888 in total from the validation_class_0_to_be_generated.csv, but first i need to caption all the images from validation_class_0_to_be_generated.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that every pristine image inside validation_class_0_to_be_generated csv is already in the image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching images: 5888\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the 'id' column from the CSV file into a set\n",
    "csv_ids = set(pd.read_csv(\"csv/validation/validation_class_0_to_be_generated.csv\")['id'])\n",
    "\n",
    "# Get a list of all the image filenames in the folder\n",
    "folder_path = \"/home/enriconello/DeepFakeDetection/dataset\"\n",
    "folder_images = set(filename.split('.')[0] for filename in os.listdir(folder_path))\n",
    "\n",
    "# Find the intersection to identify matching images\n",
    "matching_images = csv_ids.intersection(folder_images)\n",
    "\n",
    "# Count the number of matching images\n",
    "num_matching_images = len(matching_images)\n",
    "\n",
    "print(\"Number of matching images:\", num_matching_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that every pristine image inside validation_class_0_pristine csv is already in the image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching images: 11491\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the 'id' column from the CSV file into a set\n",
    "csv_ids = set(pd.read_csv(\"csv/validation/validation_class_0_pristine.csv\")['id'])\n",
    "\n",
    "# Get a list of all the image filenames in the folder\n",
    "folder_path = \"/home/enriconello/DeepFakeDetection/dataset\"\n",
    "folder_images = set(filename.split('.')[0] for filename in os.listdir(folder_path))\n",
    "\n",
    "# Find the intersection to identify matching images\n",
    "matching_images = csv_ids.intersection(folder_images)\n",
    "\n",
    "# Count the number of matching images\n",
    "num_matching_images = len(matching_images)\n",
    "\n",
    "print(\"Number of matching images:\", num_matching_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that every pristine image inside validation_class_0_already_generated csv is already in the image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching images: 5602\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the 'id' column from the CSV file into a set\n",
    "csv_ids = set(pd.read_csv(\"csv/validation/validation_class_0_already_generated.csv\")['fake_id'])\n",
    "\n",
    "# Get a list of all the image filenames in the folder\n",
    "folder_path = \"/home/enriconello/DeepFakeDetection/dataset\"\n",
    "folder_images = set(filename.split('.')[0] for filename in os.listdir(folder_path))\n",
    "\n",
    "# Find the intersection to identify matching images\n",
    "matching_images = csv_ids.intersection(folder_images)\n",
    "\n",
    "# Count the number of matching images\n",
    "num_matching_images = len(matching_images)\n",
    "\n",
    "print(\"Number of matching images:\", num_matching_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Type 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now construct the validation \"type 2\" part, thus the 8.963 pristine with 6_way_label == 1,2,3,5 (4 manipulated deleted) and 8.963 generated still with 6_way_label == 1,2,3,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4398\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('C:/Users/nello/Documents/vscode_projects/Thesis/3_image_generation/generated_images/csv/validation_synthetics.csv')\n",
    "\n",
    "# Filter rows where '6_way_label' equals 1, 2, 3, or 5 and 'fake_id' starts with 'SD', 'DL', or 'GL'\n",
    "filtered_rows = df[(df['6_way_label'].isin([1, 2, 3, 5])) & (df['fake_id'].str.startswith(('SD', 'DL', 'GL')))]\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "print(len(filtered_rows))\n",
    "filtered_rows.to_csv('csv/validation/validation_class_1_2_3_5_already_generated.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save a csv for the 8.963 pristine and the 8.963-4.398 to be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation len: 57489\n",
      "len of validation_class_1_2_3_5_already_generated: 4398\n",
      "len of validation_class_1_2_3_5_pristine: 8964\n",
      "len of validation_class_1_2_3_5_to_be_generated: 4566\n",
      "validation_class_1_2_3_5_already_generated: 4398  + validation_class_1_2_3_5_to_be_generated: 4566 = 8964\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "MULTIMODAL_VAL_CLEANED_WITH_CLASS_TSV = \"C:/Users/nello/OneDrive - University of Pisa/TESI/TSV_JSON/1_dataset_cleaning/tsv/val_tsv_with_class.tsv\"\n",
    "ALREADY_GENERATED_FROM_CLASS_1_2_3_5 = \"csv/validation/validation_class_1_2_3_5_already_generated.csv\"\n",
    "\n",
    "validation = pd.read_csv(MULTIMODAL_VAL_CLEANED_WITH_CLASS_TSV, sep='\\t')\n",
    "print(\"validation len: \"+str(len(validation)))\n",
    "validation_filtered = validation[validation['6_way_label'].isin([1, 2, 3, 5])]\n",
    "\n",
    "validation_class_1_2_3_5_already_generated = pd.read_csv(ALREADY_GENERATED_FROM_CLASS_1_2_3_5)\n",
    "print(\"len of validation_class_1_2_3_5_already_generated: \"+str(len(validation_class_1_2_3_5_already_generated)))\n",
    "\n",
    "# Filter rows from val DataFrame that are not in the 'id' column of the other DataFrame\n",
    "validation_not_in_other = validation_filtered[~validation_filtered['id'].isin(validation_class_1_2_3_5_already_generated['id'])]\n",
    "\n",
    "# Select the first 8.963 rows from validation_not_in_other\n",
    "validation_pristine = validation_not_in_other.head(8964)\n",
    "\n",
    "# Save validation_pristine to CSV\n",
    "print(\"len of validation_class_1_2_3_5_pristine: \"+str(len(validation_pristine)))\n",
    "validation_pristine.to_csv('csv/validation/validation_class_1_2_3_5_pristine.csv', index=False)\n",
    "\n",
    "# Filter rows from validation_filtered that are not in validation_class_1_2_3_5_already_generated or validation_pristine\n",
    "validation_remaining = validation_filtered[~validation_filtered['id'].isin(validation_class_1_2_3_5_already_generated['id']) & ~validation_filtered['id'].isin(validation_pristine['id'])]\n",
    "\n",
    "# Save the remaining rows to a new CSV file\n",
    "print(\"len of validation_class_1_2_3_5_to_be_generated: \"+str(len(validation_remaining)))\n",
    "validation_remaining.to_csv('csv/validation/validation_class_1_2_3_5_to_be_generated.csv', index=False)\n",
    "print(\"validation_class_1_2_3_5_already_generated: \"+str(len(validation_class_1_2_3_5_already_generated))+\"  + validation_class_1_2_3_5_to_be_generated: \"+str(len(validation_remaining))+\" = \" + str(len(validation_class_1_2_3_5_already_generated)+len(validation_remaining)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rename the columns real_image and fake_image to pristine_image and generated_image inside validation_class_1_2_3_5_already_generated.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/validation/validation_class_1_2_3_5_already_generated.csv')\n",
    "\n",
    "# Rename the column\n",
    "df.rename(columns={\"real_image\": \"pristine_image\"}, inplace=True)\n",
    "df.rename(columns={\"fake_image\": \"generated_image\"}, inplace=True)\n",
    "df.to_csv(\"csv/validation/validation_class_1_2_3_5_already_generated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also change the value of columns real_image and fake_image in validation_class_1_2_3_5_already_generated.csv from real_image=1 and fake_image=0 to real_image=0 and fake_image=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/validation/validation_class_1_2_3_5_already_generated.csv')\n",
    "\n",
    "# Swap the values of the two columns\n",
    "df['pristine_image'], df['generated_image'] = df['generated_image'], df['pristine_image']\n",
    "\n",
    "df.to_csv(\"csv/validation/validation_class_1_2_3_5_already_generated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduce columns of validation_class_1_2_3_5_to_be_generated and validation_class_1_2_3_5_pristine csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('csv/validation/validation_class_1_2_3_5_to_be_generated.csv')\n",
    "\n",
    "# Rename the column\n",
    "df.rename(columns={\"clean_title\": \"original_caption\"}, inplace=True)\n",
    "\n",
    "df['pristine_image'] = 1\n",
    "df['generated_image'] = 0\n",
    "df['real_text'] = 0\n",
    "df['fakenews_text'] = 1\n",
    "\n",
    "# Select only the desired columns in the required order\n",
    "desired_columns = ['id', 'author', 'original_caption', 'num_comments', '6_way_label', 'pristine_image', 'generated_image', 'real_text', 'fakenews_text']\n",
    "df = df[desired_columns]\n",
    "\n",
    "df.to_csv(\"csv/validation/validation_class_1_2_3_5_to_be_generated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('csv/validation/validation_class_1_2_3_5_pristine.csv')\n",
    "\n",
    "# Rename the column\n",
    "df.rename(columns={\"clean_title\": \"original_caption\"}, inplace=True)\n",
    "\n",
    "df['pristine_image'] = 1\n",
    "df['generated_image'] = 0\n",
    "df['real_text'] = 0\n",
    "df['fakenews_text'] = 1\n",
    "\n",
    "# Select only the desired columns in the required order\n",
    "desired_columns = ['id', 'author', 'original_caption', 'num_comments', '6_way_label', 'pristine_image', 'generated_image', 'real_text', 'fakenews_text']\n",
    "df = df[desired_columns]\n",
    "\n",
    "df.to_csv(\"csv/validation/validation_class_1_2_3_5_pristine.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the distribution of the original classes (1,2,3,5) for the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for each unique value of '6_way_label':\n",
      "6_way_label\n",
      "2    5570\n",
      "1    1701\n",
      "5    1082\n",
      "3     611\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('csv/validation/validation_class_1_2_3_5_pristine.csv')\n",
    "\n",
    "# Count the number of rows for each value of '6_way_label'\n",
    "label_counts = df['6_way_label'].value_counts()\n",
    "\n",
    "# Print the counts for each unique value of '6_way_label'\n",
    "print(\"Counts for each unique value of '6_way_label':\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for each unique value of '6_way_label':\n",
      "6_way_label\n",
      "2    2696\n",
      "1     890\n",
      "5     520\n",
      "3     292\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('csv/validation/validation_class_1_2_3_5_already_generated.csv')\n",
    "\n",
    "# Count the number of rows for each value of '6_way_label'\n",
    "label_counts = df['6_way_label'].value_counts()\n",
    "\n",
    "# Print the counts for each unique value of '6_way_label'\n",
    "print(\"Counts for each unique value of '6_way_label':\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for '6_way_label' 1:\n",
      "Starts with 'SD': 348\n",
      "Starts with 'GL': 186\n",
      "Starts with 'DL': 356\n",
      "\n",
      "Counts for '6_way_label' 2:\n",
      "Starts with 'SD': 1065\n",
      "Starts with 'GL': 552\n",
      "Starts with 'DL': 1079\n",
      "\n",
      "Counts for '6_way_label' 3:\n",
      "Starts with 'SD': 119\n",
      "Starts with 'GL': 50\n",
      "Starts with 'DL': 123\n",
      "\n",
      "Counts for '6_way_label' 5:\n",
      "Starts with 'SD': 205\n",
      "Starts with 'GL': 115\n",
      "Starts with 'DL': 200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('csv/validation/validation_class_1_2_3_5_already_generated.csv')\n",
    "\n",
    "# Group the DataFrame by '6_way_label'\n",
    "grouped = df.groupby('6_way_label')\n",
    "\n",
    "# Iterate over each group\n",
    "for label, group in grouped:\n",
    "    # Filter the group where 'id' column starts with \"SD\", \"GL\", or \"DL\"\n",
    "    sd_count = len(group[group['fake_id'].str.startswith('SD')])\n",
    "    gl_count = len(group[group['fake_id'].str.startswith('GL')])\n",
    "    dl_count = len(group[group['fake_id'].str.startswith('DL')])\n",
    "    \n",
    "    # Print counts for each value of '6_way_label'\n",
    "    print(f\"Counts for '6_way_label' {label}:\")\n",
    "    print(f\"Starts with 'SD': {sd_count}\")\n",
    "    print(f\"Starts with 'GL': {gl_count}\")\n",
    "    print(f\"Starts with 'DL': {dl_count}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated with SD:\n",
      "6_way_label: 1 in the interval [0, 12350): 2356\n",
      "6_way_label: 2 in the interval [0, 12350): 7614\n",
      "6_way_label: 3 in the interval [0, 12350): 860\n",
      "6_way_label: 5 in the interval [0, 12350): 1520\n",
      "\n",
      "Generated with DL:\n",
      "6_way_label: 1 in the interval [12350, 24394): 2315\n",
      "6_way_label: 2 in the interval [12350, 24394): 7437\n",
      "6_way_label: 3 in the interval [12350, 24394): 806\n",
      "6_way_label: 5 in the interval [12350, 24394): 1486\n",
      "\n",
      "Generated with GL:\n",
      "6_way_label: 1 in the interval [24394, 40997): 3146\n",
      "6_way_label: 2 in the interval [24394, 40997): 10315\n",
      "6_way_label: 3 in the interval [24394, 40997): 1080\n",
      "6_way_label: 5 in the interval [24394, 40997): 2062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('csv/validation/validation_class_1_2_3_5_to_be_generated.csv')\n",
    "\n",
    "# Define the start and end indices for each interval\n",
    "intervals = {\n",
    "    \"Generated with SD\": (0, 12350),\n",
    "    \"Generated with DL\": (12350, 24394),\n",
    "    \"Generated with GL\": (24394, len(df))\n",
    "}\n",
    "\n",
    "# Iterate over each interval\n",
    "for method, (start_index, end_index) in intervals.items():\n",
    "    # Filter the DataFrame based on the interval\n",
    "    interval_df = df.iloc[start_index:end_index]\n",
    "    \n",
    "    # Group the interval DataFrame by '6_way_label'\n",
    "    grouped = interval_df.groupby('6_way_label')\n",
    "    \n",
    "    # Print the count of rows with each '6_way_label' value within the interval\n",
    "    print(f\"{method}:\")\n",
    "    for label, group in grouped:\n",
    "        label_count = len(group)\n",
    "        print(f\"6_way_label: {label} in the interval [{start_index}, {end_index}): {label_count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if every id inside the train_tsv_with_class.csv with '6_way_label' = 1,2,3,5 is contained in the union of the 3 csv train_class_1_2_3_5_pristine + train_class_1_2_3_5_already_generated + train_class_1_2_3_5_to_be_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170774\n",
      "170774\n",
      "The union of IDs from the first three CSVs corresponds exactly to the IDs from train_tsv_with_class.csv where '6_way_label' = 1,2,3,5.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the 'id' column from the first three CSV files into sets\n",
    "csv1_ids = set(pd.read_csv(\"csv/train/train_class_1_2_3_5_pristine.csv\")['id'])\n",
    "csv2_ids = set(pd.read_csv(\"csv/train/train_class_1_2_3_5_to_be_generated.csv\")['id'])\n",
    "csv3_ids = set(pd.read_csv(\"csv/train/train_class_1_2_3_5_already_generated.csv\")['id'])\n",
    "\n",
    "# Find the union of these three sets\n",
    "union_ids = csv1_ids.union(csv2_ids, csv3_ids)\n",
    "\n",
    "csv4_ids = set(pd.read_csv(\"C:/Users/nello/OneDrive - University of Pisa/TESI/TSV_JSON/1_dataset_cleaning/tsv/train_tsv_with_class.tsv\", sep='\\t').loc[pd.read_csv(\"C:/Users/nello/OneDrive - University of Pisa/TESI/TSV_JSON/1_dataset_cleaning/tsv/train_tsv_with_class.tsv\", sep='\\t')['6_way_label'].isin([1, 2, 3, 5]), 'id'])\n",
    "\n",
    "print(len(union_ids))\n",
    "print(len(csv4_ids))\n",
    "\n",
    "# Check if the union_ids set is equal to the csv4_ids set\n",
    "if union_ids == csv4_ids:\n",
    "    print(\"The union of IDs from the first three CSVs corresponds exactly to the IDs from train_tsv_with_class.csv where '6_way_label' = 1,2,3,5.\")\n",
    "else:\n",
    "    print(\"The union of IDs from the first three CSVs does not correspond exactly to the IDs from train_tsv_with_class.csv where '6_way_label' = 1,2,3,5.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, to get to 85.387 generated images i want to generate other 40.997:\n",
    "\n",
    "- SD: 30.000 - 17.650 = 12.350\n",
    "- DL: 30.000 - 17.956 = 12.044\n",
    "- GL: (85.387 - 60.000) - 8.784 = 16.603\n",
    "\n",
    "so 40.997 in total from the train_class_1_2_3_5_to_be_generated.csv, but first i need to caption all the images from train_class_1_2_3_5_to_be_generated.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that every pristine image inside train_class_1_2_3_5_to_be_generated csv is already in the image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching images: 40997\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the 'id' column from the CSV file into a set\n",
    "csv_ids = set(pd.read_csv(\"csv/train/train_class_1_2_3_5_to_be_generated.csv\")['id'])\n",
    "\n",
    "# Get a list of all the image filenames in the folder\n",
    "folder_path = \"C:/Users/nello/Desktop/TESI/dataset_after_merging_WITH_DUPLICATES\"\n",
    "folder_images = set(filename.split('.')[0] for filename in os.listdir(folder_path))\n",
    "\n",
    "# Find the intersection to identify matching images\n",
    "matching_images = csv_ids.intersection(folder_images)\n",
    "\n",
    "# Count the number of matching images\n",
    "num_matching_images = len(matching_images)\n",
    "\n",
    "print(\"Number of matching images:\", num_matching_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that every pristine image inside train_class_1_2_3_5_pristine csv is already in the image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching images: 85387\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the 'id' column from the CSV file into a set\n",
    "csv_ids = set(pd.read_csv(\"csv/train/train_class_1_2_3_5_pristine.csv\")['id'])\n",
    "\n",
    "# Get a list of all the image filenames in the folder\n",
    "folder_path = \"/home/enriconello/DeepFakeDetection/dataset\"\n",
    "folder_images = set(filename.split('.')[0] for filename in os.listdir(folder_path))\n",
    "\n",
    "# Find the intersection to identify matching images\n",
    "matching_images = csv_ids.intersection(folder_images)\n",
    "\n",
    "# Count the number of matching images\n",
    "num_matching_images = len(matching_images)\n",
    "\n",
    "print(\"Number of matching images:\", num_matching_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that every pristine image inside train_class_1_2_3_5_already_generated csv is already in the image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching images: 44390\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the 'id' column from the CSV file into a set\n",
    "csv_ids = set(pd.read_csv(\"csv/train/train_class_1_2_3_5_already_generated.csv\")['fake_id'])\n",
    "\n",
    "# Get a list of all the image filenames in the folder\n",
    "folder_path = \"/home/enriconello/DeepFakeDetection/dataset\"\n",
    "folder_images = set(filename.split('.')[0] for filename in os.listdir(folder_path))\n",
    "\n",
    "# Find the intersection to identify matching images\n",
    "matching_images = csv_ids.intersection(folder_images)\n",
    "\n",
    "# Count the number of matching images\n",
    "num_matching_images = len(matching_images)\n",
    "\n",
    "print(\"Number of matching images:\", num_matching_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the dataset folder from Manipulated Images (class 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images removed: 190185\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "csv_files = ['/home/enriconello/DeepFakeDetection/Thesis/3_image_generation/generated_images/csv/train.csv',\n",
    "             '/home/enriconello/DeepFakeDetection/Thesis/3_image_generation/generated_images/csv/test_with_duplicates.csv',\n",
    "             '/home/enriconello/DeepFakeDetection/Thesis/3_image_generation/generated_images/csv/validation_with_duplicates.csv']\n",
    "\n",
    "# image folder path \n",
    "image_folder = '/home/enriconello/DeepFakeDetection/dataset'\n",
    "\n",
    "unique_ids = set()\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Filter rows where '6_way_label' equals to the class Manipulated images\n",
    "    filtered_df = df[df['6_way_label'] == 4]\n",
    "    unique_ids.update(filtered_df['id'])\n",
    "\n",
    "num_images_removed = 0\n",
    "\n",
    "for id_value in unique_ids:\n",
    "    image_path = os.path.join(image_folder, f\"{id_value}.jpg\")\n",
    "    \n",
    "    if os.path.exists(image_path):\n",
    "        os.remove(image_path)\n",
    "        num_images_removed += 1\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "print(f\"Total number of images removed: {num_images_removed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the train_class_1_2_3_5_captioned csv, deleting the generated captions that starts with \"araf\" (buggy captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of updated rows: 14525\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "original_df = pd.read_csv('/home/enriconello/DeepFakeDetection/Thesis/5_biased_detection/csv/train/train_class_1_2_3_5_captioned.csv')\n",
    "\n",
    "# Create a copy of the original DataFrame\n",
    "modified_df = original_df.copy()\n",
    "\n",
    "# Remove words starting with \"araf\" in the \"generated_caption\" column\n",
    "modified_df['generated_caption'] = modified_df['generated_caption'].str.replace(r'\\baraf\\w*\\b', '', regex=True)\n",
    "\n",
    "# Remove leading and trailing whitespace from the \"generated_caption\" column\n",
    "modified_df['generated_caption'] = modified_df['generated_caption'].str.strip()\n",
    "\n",
    "# Save the modified DataFrame to the same CSV file\n",
    "modified_df.to_csv(\"/home/enriconello/DeepFakeDetection/Thesis/5_biased_detection/csv/train/train_class_1_2_3_5_captioned.csv\", index=False)\n",
    "\n",
    "# Count the number of updated rows\n",
    "updated_rows_count = (original_df['generated_caption'] != modified_df['generated_caption']).sum()\n",
    "\n",
    "print(f\"Number of updated rows: {updated_rows_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After fake generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating all the fakes images that were missing, i need to merge the class_0_to_be_generated and the class_0_already_generated, and the same for class 1,2,3,5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting by adding the \"fake_id\" column to the train_class_0_to_be_generated csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/enriconello/DeepFakeDetection/Thesis/5_biased_detection/csv/train/train_class_0_to_be_generated.csv\")\n",
    "\n",
    "# new column 'fake_id'\n",
    "df['fake_id'] = ''\n",
    "\n",
    "# assign values to 'fake_id' based on row index intervals\n",
    "df.loc[df.index < 17650, 'fake_id'] = 'SD_fake_' + df['id']\n",
    "df.loc[(df.index >= 17650) & (df.index < 35606), 'fake_id'] = 'DL_fake_' + df['id']\n",
    "df.loc[df.index >= 35606, 'fake_id'] = 'GL_fake_' + df['id']\n",
    "\n",
    "# 'fake_id' column next to 'id' column\n",
    "df.insert(df.columns.get_loc('id') + 1, 'fake_id', df.pop('fake_id'))\n",
    "\n",
    "df.to_csv('/home/enriconello/DeepFakeDetection/Thesis/5_biased_detection/csv/train/train_class_0_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/enriconello/DeepFakeDetection/Thesis/5_biased_detection/csv/train/train_class_1_2_3_5_to_be_generated.csv\")\n",
    "\n",
    "# new column 'fake_id'\n",
    "df['fake_id'] = ''\n",
    "\n",
    "# assign values to 'fake_id' based on row index intervals\n",
    "df.loc[df.index < 12350, 'fake_id'] = 'SD_fake_' + df['id']\n",
    "df.loc[(df.index >= 12350) & (df.index < 24394), 'fake_id'] = 'DL_fake_' + df['id']\n",
    "df.loc[df.index >= 24394, 'fake_id'] = 'GL_fake_' + df['id']\n",
    "\n",
    "# 'fake_id' column next to 'id' column\n",
    "df.insert(df.columns.get_loc('id') + 1, 'fake_id', df.pop('fake_id'))\n",
    "\n",
    "df.to_csv('/home/enriconello/DeepFakeDetection/Thesis/5_biased_detection/csv/train/train_class_1_2_3_5_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set the column pristine_image = 0 and generated_image = 1 for every row in train_class_0_final csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/train/train_class_0_final.csv')\n",
    "\n",
    "# Swap the values of the two columns\n",
    "df['pristine_image'], df['generated_image'] = df['generated_image'], df['pristine_image']\n",
    "\n",
    "df.to_csv(\"csv/train/train_class_0_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/train/train_class_1_2_3_5_final.csv')\n",
    "\n",
    "# Swap the values of the two columns\n",
    "df['pristine_image'], df['generated_image'] = df['generated_image'], df['pristine_image']\n",
    "\n",
    "df.to_csv(\"csv/train/train_class_1_2_3_5_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now i need to add the column \"generated_caption\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fake_id</th>\n",
       "      <th>author</th>\n",
       "      <th>original_caption</th>\n",
       "      <th>generated_caption</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>6_way_label</th>\n",
       "      <th>pristine_image</th>\n",
       "      <th>generated_image</th>\n",
       "      <th>real_text</th>\n",
       "      <th>fakenews_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dhr269</td>\n",
       "      <td>SD_fake_dhr269</td>\n",
       "      <td>bico89</td>\n",
       "      <td>my bill was</td>\n",
       "      <td>someone is holding a receipt and a box of cheese</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3gb4sc</td>\n",
       "      <td>SD_fake_3gb4sc</td>\n",
       "      <td>landofthemolotovia</td>\n",
       "      <td>quebec considering removing nword from place n...</td>\n",
       "      <td>there is a bear that is walking across the water</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1wkhcb</td>\n",
       "      <td>SD_fake_1wkhcb</td>\n",
       "      <td>Zjw0115</td>\n",
       "      <td>dramatic baseball catcher</td>\n",
       "      <td>baseball player catching a ball in a stadium</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bt0p1w</td>\n",
       "      <td>SD_fake_bt0p1w</td>\n",
       "      <td>tx69er</td>\n",
       "      <td>this mcdonalds uses aol for its corporate email</td>\n",
       "      <td>a close up of a receipt with a price tag on it</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2yfbtu</td>\n",
       "      <td>SD_fake_2yfbtu</td>\n",
       "      <td>ProfessorFartdust</td>\n",
       "      <td>fan and katy perry</td>\n",
       "      <td>woman with a pink and blue hair and a man with...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id         fake_id              author  \\\n",
       "0  dhr269  SD_fake_dhr269              bico89   \n",
       "1  3gb4sc  SD_fake_3gb4sc  landofthemolotovia   \n",
       "2  1wkhcb  SD_fake_1wkhcb             Zjw0115   \n",
       "3  bt0p1w  SD_fake_bt0p1w              tx69er   \n",
       "4  2yfbtu  SD_fake_2yfbtu   ProfessorFartdust   \n",
       "\n",
       "                                    original_caption  \\\n",
       "0                                        my bill was   \n",
       "1  quebec considering removing nword from place n...   \n",
       "2                          dramatic baseball catcher   \n",
       "3    this mcdonalds uses aol for its corporate email   \n",
       "4                                 fan and katy perry   \n",
       "\n",
       "                                   generated_caption  num_comments  \\\n",
       "0   someone is holding a receipt and a box of cheese          10.0   \n",
       "1   there is a bear that is walking across the water          19.0   \n",
       "2       baseball player catching a ball in a stadium           6.0   \n",
       "3     a close up of a receipt with a price tag on it           3.0   \n",
       "4  woman with a pink and blue hair and a man with...           5.0   \n",
       "\n",
       "   6_way_label  pristine_image  generated_image  real_text  fakenews_text  \n",
       "0            0               0                1          1              0  \n",
       "1            0               0                1          1              0  \n",
       "2            0               0                1          1              0  \n",
       "3            0               0                1          1              0  \n",
       "4            0               0                1          1              0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_a = pd.read_csv(\"csv/train/train_class_0_final.csv\")\n",
    "df_b = pd.read_csv(\"csv/train/train_class_0_captioned.csv\")\n",
    "\n",
    "# Perform inner join on 'id' column\n",
    "df_c = pd.merge(df_a, df_b, on='id', how='inner')\n",
    "\n",
    "# Rename 'original_caption_x' to 'original_caption'\n",
    "df_c = df_c.rename(columns={'original_caption_x': 'original_caption'})\n",
    "\n",
    "df_c = df_c[['id', 'fake_id', 'author', 'original_caption', 'generated_caption', 'num_comments', '6_way_label', 'pristine_image', 'generated_image', 'real_text', 'fakenews_text']]\n",
    "\n",
    "df_c.to_csv(\"csv/train/train_class_0_final.csv\", index=False)\n",
    "df_c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_a = pd.read_csv(\"csv/train/train_class_1_2_3_5_final.csv\")\n",
    "df_b = pd.read_csv(\"csv/train/train_class_1_2_3_5_captioned.csv\")\n",
    "\n",
    "# Perform inner join on 'id' column\n",
    "df_c = pd.merge(df_a, df_b, on='id', how='inner')\n",
    "\n",
    "# Rename 'original_caption_x' to 'original_caption'\n",
    "df_c = df_c.rename(columns={'original_caption_x': 'original_caption'})\n",
    "\n",
    "df_c = df_c[['id', 'fake_id', 'author', 'original_caption', 'generated_caption', 'num_comments', '6_way_label', 'pristine_image', 'generated_image', 'real_text', 'fakenews_text']]\n",
    "\n",
    "df_c.to_csv(\"csv/train/train_class_1_2_3_5_final2.csv\", index=False)\n",
    "df_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally merge train_class_0_final and train_class_1_2_3_5_final to create train_final csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read CSV files\n",
    "df1 = pd.read_csv(\"csv/train/train_class_0_final.csv\")\n",
    "df2 = pd.read_csv(\"csv/train/train_class_1_2_3_5_final.csv\")\n",
    "\n",
    "# Concatenate DataFrames\n",
    "df_union = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save the result to CSV file\n",
    "df_union.to_csv(\"csv/train/train_final.csv\", index=False)\n",
    "print(len(df_union))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
