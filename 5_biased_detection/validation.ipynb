{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Type 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VALIDATION CLASS 0: collect the fake images already generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5602\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('C:/Users/nello/Documents/vscode_projects/Thesis/3_image_generation/generated_images/csv/validation_synthetics.csv')\n",
    "\n",
    "# Filter rows where '6_way_label' equals 0 and 'fake_id' starts with 'SD', 'DL', or 'GL'\n",
    "filtered_rows_SD = df[(df['6_way_label'] == 0) & (df['fake_id'].str.startswith('SD'))]\n",
    "filtered_rows_DL = df[(df['6_way_label'] == 0) & (df['fake_id'].str.startswith('DL'))]\n",
    "filtered_rows_GL = df[(df['6_way_label'] == 0) & (df['fake_id'].str.startswith('GL'))]\n",
    "\n",
    "# Concatenate columns for all groups\n",
    "result = pd.concat([filtered_rows_SD, filtered_rows_DL, filtered_rows_GL])\n",
    "print(len(result))\n",
    "\n",
    "result.to_csv('C:/Users/nello/Documents/vscode_projects/Thesis/5_biased_detection/csv/validation/validation_class_0_already_generated.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save a csv for the 11.490 pristine and the 11.490 - 5.602 to be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of validation_class_0_already_generated: 5602\n",
      "len of validation_class_0_pristine: 11491\n",
      "len of validation_class_0_to_be_generated: 5888\n",
      "validation_class_0_already_generated: 5602 + validation_class_0_to_be_generated: 5888 = 11490\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "MULTIMODAL_VAL_CLEANED_WITH_CLASS_TSV = \"C:/Users/nello/OneDrive - University of Pisa/TESI/TSV_JSON/1_dataset_cleaning/tsv/val_tsv_with_class.tsv\"\n",
    "ALREADY_GENERATED_FROM_CLASS_0 = \"csv/validation/validation_class_0_already_generated.csv\"\n",
    "\n",
    "validation = pd.read_csv(MULTIMODAL_VAL_CLEANED_WITH_CLASS_TSV, sep='\\t')\n",
    "validation_filtered = validation[validation['6_way_label'] == 0]\n",
    "\n",
    "validation_class_0_already_generated = pd.read_csv(ALREADY_GENERATED_FROM_CLASS_0)\n",
    "print(\"len of validation_class_0_already_generated: \"+str(len(validation_class_0_already_generated)))\n",
    "\n",
    "# Filter rows from train DataFrame that are not in the 'id' column of the other DataFrame\n",
    "validation_not_in_other = validation_filtered[~validation_filtered['id'].isin(validation_class_0_already_generated['id'])]\n",
    "\n",
    "# Select the first 11,490 rows from train_not_in_other\n",
    "validation_pristine = validation_not_in_other.head(11491)\n",
    "\n",
    "# Save train_pristine to CSV\n",
    "print(\"len of validation_class_0_pristine: \"+str(len(validation_pristine)))\n",
    "validation_pristine.to_csv('csv/validation/validation_class_0_pristine.csv', index=False)\n",
    "\n",
    "# Filter rows from train_filtered that are not in train_class_0_already_generated or train_pristine\n",
    "validation_remaining = validation_filtered[~validation_filtered['id'].isin(validation_class_0_already_generated['id']) & ~validation_filtered['id'].isin(validation_pristine['id'])]\n",
    "\n",
    "# Save the remaining rows to a new CSV file\n",
    "print(\"len of validation_class_0_to_be_generated: \"+str(len(validation_remaining)))\n",
    "validation_remaining.to_csv('csv/validation/validation_class_0_to_be_generated.csv', index=False)\n",
    "print(\"validation_class_0_already_generated: \"+str(len(validation_class_0_already_generated))+\" + validation_class_0_to_be_generated: \"+str(len(validation_remaining))+\" = \" + str(len(validation_class_0_already_generated)+len(validation_remaining)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rename the columns real_image and fake_image to pristine_image and generated_image inside validation_class_0_already_generated.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/validation/validation_class_0_already_generated.csv')\n",
    "\n",
    "# Rename the column\n",
    "df.rename(columns={\"real_image\": \"pristine_image\"}, inplace=True)\n",
    "df.rename(columns={\"fake_image\": \"generated_image\"}, inplace=True)\n",
    "df.to_csv(\"csv/validation/validation_class_0_already_generated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also change the value of columns real_image and fake_image in validation_class_0_already_generated.csv from real_image=1 and fake_image=0 to real_image=0 and fake_image=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/validation/validation_class_0_already_generated.csv')\n",
    "\n",
    "# Swap the values of the two columns\n",
    "df['pristine_image'], df['generated_image'] = df['generated_image'], df['pristine_image']\n",
    "\n",
    "df.to_csv(\"csv/validation/validation_class_0_already_generated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduce columns of validation_class_0_to_be_generated and validation_class_0_pristine csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('csv/validation/validation_class_0_to_be_generated.csv')\n",
    "\n",
    "# Rename the column\n",
    "df.rename(columns={\"clean_title\": \"original_caption\"}, inplace=True)\n",
    "\n",
    "df['pristine_image'] = 1\n",
    "df['generated_image'] = 0\n",
    "df['real_text'] = 1\n",
    "df['fakenews_text'] = 0\n",
    "\n",
    "# Select only the desired columns in the required order\n",
    "desired_columns = ['id', 'author', 'original_caption', 'num_comments', '6_way_label', 'pristine_image', 'generated_image', 'real_text', 'fakenews_text']\n",
    "df = df[desired_columns]\n",
    "\n",
    "df.to_csv(\"csv/validation/validation_class_0_to_be_generated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('csv/validation/validation_class_0_pristine.csv')\n",
    "\n",
    "# Rename the column\n",
    "df.rename(columns={\"clean_title\": \"original_caption\"}, inplace=True)\n",
    "\n",
    "df['pristine_image'] = 1\n",
    "df['generated_image'] = 0\n",
    "df['real_text'] = 1\n",
    "df['fakenews_text'] = 0\n",
    "\n",
    "# Select only the desired columns in the required order\n",
    "desired_columns = ['id', 'author', 'original_caption', 'num_comments', '6_way_label', 'pristine_image', 'generated_image', 'real_text', 'fakenews_text']\n",
    "df = df[desired_columns]\n",
    "\n",
    "df.to_csv(\"csv/validation/validation_class_0_pristine.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if every id inside the validation_tsv_with_class.csv is contained in the union of the 3 csv validation_pristine + validation_class_0_already_generated + validation_class_0_to_be_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22981\n",
      "22981\n",
      "The union of IDs from the first three CSVs corresponds exactly to the IDs from validation_tsv_with_class.csv where '6_way_label' = 0.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the 'id' column from the first three CSV files into sets\n",
    "csv1_ids = set(pd.read_csv(\"csv/validation/validation_class_0_pristine.csv\")['id'])\n",
    "csv2_ids = set(pd.read_csv(\"csv/validation/validation_class_0_to_be_generated.csv\")['id'])\n",
    "csv3_ids = set(pd.read_csv(\"csv/validation/validation_class_0_already_generated.csv\")['id'])\n",
    "\n",
    "# Find the union of these three sets\n",
    "union_ids = csv1_ids.union(csv2_ids, csv3_ids)\n",
    "\n",
    "csv4_ids = set(pd.read_csv(\"C:/Users/nello/OneDrive - University of Pisa/TESI/TSV_JSON/1_dataset_cleaning/tsv/val_tsv_with_class.tsv\", sep='\\t').loc[pd.read_csv(\"C:/Users/nello/OneDrive - University of Pisa/TESI/TSV_JSON/1_dataset_cleaning/tsv/val_tsv_with_class.tsv\", sep='\\t')['6_way_label'] == 0, 'id'])\n",
    "\n",
    "print(len(union_ids))\n",
    "print(len(csv4_ids))\n",
    "\n",
    "# Check if the union_ids set is equal to the csv4_ids set\n",
    "if union_ids == csv4_ids:\n",
    "    print(\"The union of IDs from the first three CSVs corresponds exactly to the IDs from validation_tsv_with_class.csv where '6_way_label' = 0.\")\n",
    "else:\n",
    "    print(\"The union of IDs from the first three CSVs does not correspond exactly to the IDs from validation_tsv_with_class.csv where '6_way_label' = 0.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now i print out how many StableDiffusion, Dreamlike and Glide images i have already generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where 'fake_id' starts with 'SD': 2263\n",
      "Number of rows where 'fake_id' starts with 'DL': 2242\n",
      "Number of rows where 'fake_id' starts with 'GL': 1097\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"csv/validation/validation_class_0_already_generated.csv\")\n",
    "\n",
    "# Count the rows where the 'fake_id' column starts with 'SD', 'DL', or 'GL'\n",
    "count_SD = len(df[df['fake_id'].str.startswith('SD')])\n",
    "count_DL = len(df[df['fake_id'].str.startswith('DL')])\n",
    "count_GL = len(df[df['fake_id'].str.startswith('GL')])\n",
    "\n",
    "print(\"Number of rows where 'fake_id' starts with 'SD':\", count_SD)\n",
    "print(\"Number of rows where 'fake_id' starts with 'DL':\", count_DL)\n",
    "print(\"Number of rows where 'fake_id' starts with 'GL':\", count_GL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, to get to 11.490 generated images i want to generate other:\n",
    "\n",
    "- SD: 4.000 - 2.263 = 1.737\n",
    "- DL: 4.000 - 2.242 = 1.758\n",
    "- GL: (11.490 - 8.000) - 1.097 = 2.393\n",
    "\n",
    "so 5.888 in total from the validation_class_0_to_be_generated.csv, but first i need to caption all the images from validation_class_0_to_be_generated.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that every pristine image inside validation_class_0_to_be_generated csv is already in the image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching images: 5888\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the 'id' column from the CSV file into a set\n",
    "csv_ids = set(pd.read_csv(\"csv/validation/validation_class_0_to_be_generated.csv\")['id'])\n",
    "\n",
    "# Get a list of all the image filenames in the folder\n",
    "folder_path = \"/home/enriconello/DeepFakeDetection/dataset\"\n",
    "folder_images = set(filename.split('.')[0] for filename in os.listdir(folder_path))\n",
    "\n",
    "# Find the intersection to identify matching images\n",
    "matching_images = csv_ids.intersection(folder_images)\n",
    "\n",
    "# Count the number of matching images\n",
    "num_matching_images = len(matching_images)\n",
    "\n",
    "print(\"Number of matching images:\", num_matching_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that every pristine image inside validation_class_0_pristine csv is already in the image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching images: 11491\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the 'id' column from the CSV file into a set\n",
    "csv_ids = set(pd.read_csv(\"csv/validation/validation_class_0_pristine.csv\")['id'])\n",
    "\n",
    "# Get a list of all the image filenames in the folder\n",
    "folder_path = \"/home/enriconello/DeepFakeDetection/dataset\"\n",
    "folder_images = set(filename.split('.')[0] for filename in os.listdir(folder_path))\n",
    "\n",
    "# Find the intersection to identify matching images\n",
    "matching_images = csv_ids.intersection(folder_images)\n",
    "\n",
    "# Count the number of matching images\n",
    "num_matching_images = len(matching_images)\n",
    "\n",
    "print(\"Number of matching images:\", num_matching_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that every pristine image inside validation_class_0_already_generated csv is already in the image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching images: 5602\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the 'id' column from the CSV file into a set\n",
    "csv_ids = set(pd.read_csv(\"csv/validation/validation_class_0_already_generated.csv\")['fake_id'])\n",
    "\n",
    "# Get a list of all the image filenames in the folder\n",
    "folder_path = \"/home/enriconello/DeepFakeDetection/dataset\"\n",
    "folder_images = set(filename.split('.')[0] for filename in os.listdir(folder_path))\n",
    "\n",
    "# Find the intersection to identify matching images\n",
    "matching_images = csv_ids.intersection(folder_images)\n",
    "\n",
    "# Count the number of matching images\n",
    "num_matching_images = len(matching_images)\n",
    "\n",
    "print(\"Number of matching images:\", num_matching_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the validation_class_0_captioned csv, deleting the generated captions that starts with \"araf\" (buggy captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "original_df = pd.read_csv('/home/enriconello/DeepFakeDetection/Thesis/5_biased_detection/csv/validation/validation_class_0_captioned.csv')\n",
    "\n",
    "# Create a copy of the original DataFrame\n",
    "modified_df = original_df.copy()\n",
    "\n",
    "# Remove words starting with \"araf\" in the \"generated_caption\" column\n",
    "modified_df['generated_caption'] = modified_df['generated_caption'].str.replace(r'\\baraf\\w*\\b', '', regex=True)\n",
    "\n",
    "# Remove leading and trailing whitespace from the \"generated_caption\" column\n",
    "modified_df['generated_caption'] = modified_df['generated_caption'].str.strip()\n",
    "\n",
    "# Save the modified DataFrame to the same CSV file\n",
    "modified_df.to_csv(\"/home/enriconello/DeepFakeDetection/Thesis/5_biased_detection/csv/validation/validation_class_0_captioned.csv\", index=False)\n",
    "\n",
    "# Count the number of updated rows\n",
    "updated_rows_count = (original_df['generated_caption'] != modified_df['generated_caption']).sum()\n",
    "\n",
    "print(f\"Number of updated rows: {updated_rows_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Type 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now construct the validation \"type 2\" part, thus the 8.963 pristine with 6_way_label == 1,2,3,5 (4 manipulated deleted) and 8.963 generated still with 6_way_label == 1,2,3,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4398\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('C:/Users/nello/Documents/vscode_projects/Thesis/3_image_generation/generated_images/csv/validation_synthetics.csv')\n",
    "\n",
    "# Filter rows where '6_way_label' equals 1, 2, 3, or 5 and 'fake_id' starts with 'SD', 'DL', or 'GL'\n",
    "filtered_rows = df[(df['6_way_label'].isin([1, 2, 3, 5])) & (df['fake_id'].str.startswith(('SD', 'DL', 'GL')))]\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "print(len(filtered_rows))\n",
    "filtered_rows.to_csv('csv/validation/validation_class_1_2_3_5_already_generated.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save a csv for the 8.963 pristine and the 8.963-4.398 to be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation len: 57489\n",
      "len of validation_class_1_2_3_5_already_generated: 4398\n",
      "len of validation_class_1_2_3_5_pristine: 8964\n",
      "len of validation_class_1_2_3_5_to_be_generated: 4566\n",
      "validation_class_1_2_3_5_already_generated: 4398  + validation_class_1_2_3_5_to_be_generated: 4566 = 8964\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "MULTIMODAL_VAL_CLEANED_WITH_CLASS_TSV = \"C:/Users/nello/OneDrive - University of Pisa/TESI/TSV_JSON/1_dataset_cleaning/tsv/val_tsv_with_class.tsv\"\n",
    "ALREADY_GENERATED_FROM_CLASS_1_2_3_5 = \"csv/validation/validation_class_1_2_3_5_already_generated.csv\"\n",
    "\n",
    "validation = pd.read_csv(MULTIMODAL_VAL_CLEANED_WITH_CLASS_TSV, sep='\\t')\n",
    "print(\"validation len: \"+str(len(validation)))\n",
    "validation_filtered = validation[validation['6_way_label'].isin([1, 2, 3, 5])]\n",
    "\n",
    "validation_class_1_2_3_5_already_generated = pd.read_csv(ALREADY_GENERATED_FROM_CLASS_1_2_3_5)\n",
    "print(\"len of validation_class_1_2_3_5_already_generated: \"+str(len(validation_class_1_2_3_5_already_generated)))\n",
    "\n",
    "# Filter rows from val DataFrame that are not in the 'id' column of the other DataFrame\n",
    "validation_not_in_other = validation_filtered[~validation_filtered['id'].isin(validation_class_1_2_3_5_already_generated['id'])]\n",
    "\n",
    "# Select the first 8.963 rows from validation_not_in_other\n",
    "validation_pristine = validation_not_in_other.head(8964)\n",
    "\n",
    "# Save validation_pristine to CSV\n",
    "print(\"len of validation_class_1_2_3_5_pristine: \"+str(len(validation_pristine)))\n",
    "validation_pristine.to_csv('csv/validation/validation_class_1_2_3_5_pristine.csv', index=False)\n",
    "\n",
    "# Filter rows from validation_filtered that are not in validation_class_1_2_3_5_already_generated or validation_pristine\n",
    "validation_remaining = validation_filtered[~validation_filtered['id'].isin(validation_class_1_2_3_5_already_generated['id']) & ~validation_filtered['id'].isin(validation_pristine['id'])]\n",
    "\n",
    "# Save the remaining rows to a new CSV file\n",
    "print(\"len of validation_class_1_2_3_5_to_be_generated: \"+str(len(validation_remaining)))\n",
    "validation_remaining.to_csv('csv/validation/validation_class_1_2_3_5_to_be_generated.csv', index=False)\n",
    "print(\"validation_class_1_2_3_5_already_generated: \"+str(len(validation_class_1_2_3_5_already_generated))+\"  + validation_class_1_2_3_5_to_be_generated: \"+str(len(validation_remaining))+\" = \" + str(len(validation_class_1_2_3_5_already_generated)+len(validation_remaining)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rename the columns real_image and fake_image to pristine_image and generated_image inside validation_class_1_2_3_5_already_generated.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/validation/validation_class_1_2_3_5_already_generated.csv')\n",
    "\n",
    "# Rename the column\n",
    "df.rename(columns={\"real_image\": \"pristine_image\"}, inplace=True)\n",
    "df.rename(columns={\"fake_image\": \"generated_image\"}, inplace=True)\n",
    "df.to_csv(\"csv/validation/validation_class_1_2_3_5_already_generated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also change the value of columns real_image and fake_image in validation_class_1_2_3_5_already_generated.csv from real_image=1 and fake_image=0 to real_image=0 and fake_image=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/validation/validation_class_1_2_3_5_already_generated.csv')\n",
    "\n",
    "# Swap the values of the two columns\n",
    "df['pristine_image'], df['generated_image'] = df['generated_image'], df['pristine_image']\n",
    "\n",
    "df.to_csv(\"csv/validation/validation_class_1_2_3_5_already_generated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduce columns of validation_class_1_2_3_5_to_be_generated and validation_class_1_2_3_5_pristine csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('csv/validation/validation_class_1_2_3_5_to_be_generated.csv')\n",
    "\n",
    "# Rename the column\n",
    "df.rename(columns={\"clean_title\": \"original_caption\"}, inplace=True)\n",
    "\n",
    "df['pristine_image'] = 1\n",
    "df['generated_image'] = 0\n",
    "df['real_text'] = 0\n",
    "df['fakenews_text'] = 1\n",
    "\n",
    "# Select only the desired columns in the required order\n",
    "desired_columns = ['id', 'author', 'original_caption', 'num_comments', '6_way_label', 'pristine_image', 'generated_image', 'real_text', 'fakenews_text']\n",
    "df = df[desired_columns]\n",
    "\n",
    "df.to_csv(\"csv/validation/validation_class_1_2_3_5_to_be_generated.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('csv/validation/validation_class_1_2_3_5_pristine.csv')\n",
    "\n",
    "# Rename the column\n",
    "df.rename(columns={\"clean_title\": \"original_caption\"}, inplace=True)\n",
    "\n",
    "df['pristine_image'] = 1\n",
    "df['generated_image'] = 0\n",
    "df['real_text'] = 0\n",
    "df['fakenews_text'] = 1\n",
    "\n",
    "# Select only the desired columns in the required order\n",
    "desired_columns = ['id', 'author', 'original_caption', 'num_comments', '6_way_label', 'pristine_image', 'generated_image', 'real_text', 'fakenews_text']\n",
    "df = df[desired_columns]\n",
    "\n",
    "df.to_csv(\"csv/validation/validation_class_1_2_3_5_pristine.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the distribution of the original classes (1,2,3,5) for the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for each unique value of '6_way_label':\n",
      "6_way_label\n",
      "2    5570\n",
      "1    1701\n",
      "5    1082\n",
      "3     611\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('csv/validation/validation_class_1_2_3_5_pristine.csv')\n",
    "\n",
    "# Count the number of rows for each value of '6_way_label'\n",
    "label_counts = df['6_way_label'].value_counts()\n",
    "\n",
    "# Print the counts for each unique value of '6_way_label'\n",
    "print(\"Counts for each unique value of '6_way_label':\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for each unique value of '6_way_label':\n",
      "6_way_label\n",
      "2    2696\n",
      "1     890\n",
      "5     520\n",
      "3     292\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('csv/validation/validation_class_1_2_3_5_already_generated.csv')\n",
    "\n",
    "# Count the number of rows for each value of '6_way_label'\n",
    "label_counts = df['6_way_label'].value_counts()\n",
    "\n",
    "# Print the counts for each unique value of '6_way_label'\n",
    "print(\"Counts for each unique value of '6_way_label':\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for '6_way_label' 1:\n",
      "Starts with 'SD': 348\n",
      "Starts with 'GL': 186\n",
      "Starts with 'DL': 356\n",
      "\n",
      "Counts for '6_way_label' 2:\n",
      "Starts with 'SD': 1065\n",
      "Starts with 'GL': 552\n",
      "Starts with 'DL': 1079\n",
      "\n",
      "Counts for '6_way_label' 3:\n",
      "Starts with 'SD': 119\n",
      "Starts with 'GL': 50\n",
      "Starts with 'DL': 123\n",
      "\n",
      "Counts for '6_way_label' 5:\n",
      "Starts with 'SD': 205\n",
      "Starts with 'GL': 115\n",
      "Starts with 'DL': 200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('csv/validation/validation_class_1_2_3_5_already_generated.csv')\n",
    "\n",
    "# Group the DataFrame by '6_way_label'\n",
    "grouped = df.groupby('6_way_label')\n",
    "\n",
    "# Iterate over each group\n",
    "for label, group in grouped:\n",
    "    # Filter the group where 'id' column starts with \"SD\", \"GL\", or \"DL\"\n",
    "    sd_count = len(group[group['fake_id'].str.startswith('SD')])\n",
    "    gl_count = len(group[group['fake_id'].str.startswith('GL')])\n",
    "    dl_count = len(group[group['fake_id'].str.startswith('DL')])\n",
    "    \n",
    "    # Print counts for each value of '6_way_label'\n",
    "    print(f\"Counts for '6_way_label' {label}:\")\n",
    "    print(f\"Starts with 'SD': {sd_count}\")\n",
    "    print(f\"Starts with 'GL': {gl_count}\")\n",
    "    print(f\"Starts with 'DL': {dl_count}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of rows starting with 'SD': 1737\n",
      "Count of rows starting with 'DL': 1758\n",
      "Count of rows starting with 'GL': 903\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('csv/validation/validation_class_1_2_3_5_already_generated.csv')\n",
    "\n",
    "# Calculate the count of rows starting with 'SD', 'DL', and 'GL'\n",
    "sd_count = len(df[df['fake_id'].str.startswith('SD')])\n",
    "dl_count = len(df[df['fake_id'].str.startswith('DL')])\n",
    "gl_count = len(df[df['fake_id'].str.startswith('GL')])\n",
    "\n",
    "# Print counts for each case\n",
    "print(f\"Count of rows starting with 'SD': {sd_count}\")\n",
    "print(f\"Count of rows starting with 'DL': {dl_count}\")\n",
    "print(f\"Count of rows starting with 'GL': {gl_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, to get to 8.963 generated images i want to generate other 8.963-4398 = 4.565:\n",
    "\n",
    "- SD: 3.000 - 1.737 = 1.263\n",
    "- DL: 3.000 - 1.758 = 1.242\n",
    "- GL: (8.963 - 6.000) - 903 = 2.060\n",
    "\n",
    "(3.000 means i want 3.000 images fakes from SD out of 8.963 total fakes)\n",
    "\n",
    "so 4.565 in total from the validation_class_1_2_3_5_to_be_generated.csv, but first i need to caption all the images from validation_class_1_2_3_5_to_be_generated.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated with SD:\n",
      "6_way_label: 1 in the interval [0, 1263): 247\n",
      "6_way_label: 2 in the interval [0, 1263): 769\n",
      "6_way_label: 3 in the interval [0, 1263): 87\n",
      "6_way_label: 5 in the interval [0, 1263): 160\n",
      "\n",
      "Generated with DL:\n",
      "6_way_label: 1 in the interval [1263, 2505): 225\n",
      "6_way_label: 2 in the interval [1263, 2505): 782\n",
      "6_way_label: 3 in the interval [1263, 2505): 76\n",
      "6_way_label: 5 in the interval [1263, 2505): 159\n",
      "\n",
      "Generated with GL:\n",
      "6_way_label: 1 in the interval [2505, 4566): 375\n",
      "6_way_label: 2 in the interval [2505, 4566): 1299\n",
      "6_way_label: 3 in the interval [2505, 4566): 143\n",
      "6_way_label: 5 in the interval [2505, 4566): 244\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('csv/validation/validation_class_1_2_3_5_to_be_generated.csv')\n",
    "\n",
    "# Define the start and end indices for each interval\n",
    "intervals = {\n",
    "    \"Generated with SD\": (0, 1263),\n",
    "    \"Generated with DL\": (1263, 2505),\n",
    "    \"Generated with GL\": (2505, len(df))\n",
    "}\n",
    "\n",
    "# Iterate over each interval\n",
    "for method, (start_index, end_index) in intervals.items():\n",
    "    # Filter the DataFrame based on the interval\n",
    "    interval_df = df.iloc[start_index:end_index]\n",
    "    \n",
    "    # Group the interval DataFrame by '6_way_label'\n",
    "    grouped = interval_df.groupby('6_way_label')\n",
    "    \n",
    "    # Print the count of rows with each '6_way_label' value within the interval\n",
    "    print(f\"{method}:\")\n",
    "    for label, group in grouped:\n",
    "        label_count = len(group)\n",
    "        print(f\"6_way_label: {label} in the interval [{start_index}, {end_index}): {label_count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if every id inside the validation_tsv_with_class.csv with '6_way_label' = 1,2,3,5 is contained in the union of the 3 csv validation_class_1_2_3_5_pristine + validation_class_1_2_3_5_already_generated + validation_class_1_2_3_5_to_be_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170774\n",
      "170774\n",
      "The union of IDs from the first three CSVs corresponds exactly to the IDs from train_tsv_with_class.csv where '6_way_label' = 1,2,3,5.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the 'id' column from the first three CSV files into sets\n",
    "csv1_ids = set(pd.read_csv(\"csv/validation/validation_class_1_2_3_5_pristine.csv\")['id'])\n",
    "csv2_ids = set(pd.read_csv(\"csv/validation/validation_class_1_2_3_5_to_be_generated.csv\")['id'])\n",
    "csv3_ids = set(pd.read_csv(\"csv/validation/validation_class_1_2_3_5_already_generated.csv\")['id'])\n",
    "\n",
    "# Find the union of these three sets\n",
    "union_ids = csv1_ids.union(csv2_ids, csv3_ids)\n",
    "\n",
    "csv4_ids = set(pd.read_csv(\"C:/Users/nello/OneDrive - University of Pisa/TESI/TSV_JSON/1_dataset_cleaning/tsv/val_tsv_with_class.tsv\", sep='\\t').loc[pd.read_csv(\"C:/Users/nello/OneDrive - University of Pisa/TESI/TSV_JSON/1_dataset_cleaning/tsv/train_tsv_with_class.tsv\", sep='\\t')['6_way_label'].isin([1, 2, 3, 5]), 'id'])\n",
    "\n",
    "print(len(union_ids))\n",
    "print(len(csv4_ids))\n",
    "\n",
    "# Check if the union_ids set is equal to the csv4_ids set\n",
    "if union_ids == csv4_ids:\n",
    "    print(\"The union of IDs from the first three CSVs corresponds exactly to the IDs from validation_tsv_with_class.csv where '6_way_label' = 1,2,3,5.\")\n",
    "else:\n",
    "    print(\"The union of IDs from the first three CSVs does not correspond exactly to the IDs from validation_tsv_with_class.csv where '6_way_label' = 1,2,3,5.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that every pristine image inside validation_class_1_2_3_5_to_be_generated csv is already in the image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching images: 4566\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the 'id' column from the CSV file into a set\n",
    "csv_ids = set(pd.read_csv(\"csv/validation/validation_class_1_2_3_5_to_be_generated.csv\")['id'])\n",
    "\n",
    "# Get a list of all the image filenames in the folder\n",
    "folder_path = \"/home/enriconello/DeepFakeDetection/dataset\"\n",
    "folder_images = set(filename.split('.')[0] for filename in os.listdir(folder_path))\n",
    "\n",
    "# Find the intersection to identify matching images\n",
    "matching_images = csv_ids.intersection(folder_images)\n",
    "\n",
    "# Count the number of matching images\n",
    "num_matching_images = len(matching_images)\n",
    "\n",
    "print(\"Number of matching images:\", num_matching_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that every pristine image inside validation_class_1_2_3_5_pristine csv is already in the image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching images: 8964\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the 'id' column from the CSV file into a set\n",
    "csv_ids = set(pd.read_csv(\"csv/validation/validation_class_1_2_3_5_pristine.csv\")['id'])\n",
    "\n",
    "# Get a list of all the image filenames in the folder\n",
    "folder_path = \"/home/enriconello/DeepFakeDetection/dataset\"\n",
    "folder_images = set(filename.split('.')[0] for filename in os.listdir(folder_path))\n",
    "\n",
    "# Find the intersection to identify matching images\n",
    "matching_images = csv_ids.intersection(folder_images)\n",
    "\n",
    "# Count the number of matching images\n",
    "num_matching_images = len(matching_images)\n",
    "\n",
    "print(\"Number of matching images:\", num_matching_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we check that every pristine image inside validation_class_1_2_3_5_already_generated csv is already in the image folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching images: 4398\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the 'id' column from the CSV file into a set\n",
    "csv_ids = set(pd.read_csv(\"csv/validation/validation_class_1_2_3_5_already_generated.csv\")['fake_id'])\n",
    "\n",
    "# Get a list of all the image filenames in the folder\n",
    "folder_path = \"/home/enriconello/DeepFakeDetection/dataset\"\n",
    "folder_images = set(filename.split('.')[0] for filename in os.listdir(folder_path))\n",
    "\n",
    "# Find the intersection to identify matching images\n",
    "matching_images = csv_ids.intersection(folder_images)\n",
    "\n",
    "# Count the number of matching images\n",
    "num_matching_images = len(matching_images)\n",
    "\n",
    "print(\"Number of matching images:\", num_matching_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the validation_class_1_2_3_5_captioned csv, deleting the generated captions that starts with \"araf\" (buggy captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of updated rows: 1528\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "original_df = pd.read_csv('/home/enriconello/DeepFakeDetection/Thesis/5_biased_detection/csv/validation/validation_class_1_2_3_5_captioned.csv')\n",
    "\n",
    "# Create a copy of the original DataFrame\n",
    "modified_df = original_df.copy()\n",
    "\n",
    "# Remove words starting with \"araf\" in the \"generated_caption\" column\n",
    "modified_df['generated_caption'] = modified_df['generated_caption'].str.replace(r'\\baraf\\w*\\b', '', regex=True)\n",
    "\n",
    "# Remove leading and trailing whitespace from the \"generated_caption\" column\n",
    "modified_df['generated_caption'] = modified_df['generated_caption'].str.strip()\n",
    "\n",
    "# Save the modified DataFrame to the same CSV file\n",
    "modified_df.to_csv(\"/home/enriconello/DeepFakeDetection/Thesis/5_biased_detection/csv/validation/validation_class_1_2_3_5_captioned.csv\", index=False)\n",
    "\n",
    "# Count the number of updated rows\n",
    "updated_rows_count = (original_df['generated_caption'] != modified_df['generated_caption']).sum()\n",
    "\n",
    "print(f\"Number of updated rows: {updated_rows_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After fake generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating all the fakes images that were missing, i need to merge the class_0_to_be_generated and the class_0_already_generated, and the same for class 1,2,3,5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting by adding the \"fake_id\" column to the validation_class_0_to_be_generated csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/enriconello/DeepFakeDetection/Thesis/5_biased_detection/csv/validation/validation_class_0_to_be_generated.csv\")\n",
    "\n",
    "# new column 'fake_id'\n",
    "df['fake_id'] = ''\n",
    "\n",
    "# assign values to 'fake_id' based on row index intervals\n",
    "df.loc[df.index < 1737, 'fake_id'] = 'SD_fake_' + df['id']\n",
    "df.loc[(df.index >= 1737) & (df.index < 3495), 'fake_id'] = 'DL_fake_' + df['id']\n",
    "df.loc[df.index >= 3495, 'fake_id'] = 'GL_fake_' + df['id']\n",
    "\n",
    "# 'fake_id' column next to 'id' column\n",
    "df.insert(df.columns.get_loc('id') + 1, 'fake_id', df.pop('fake_id'))\n",
    "\n",
    "df.to_csv('/home/enriconello/DeepFakeDetection/Thesis/5_biased_detection/csv/validation/validation_class_0_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/enriconello/DeepFakeDetection/Thesis/5_biased_detection/csv/validation/validation_class_1_2_3_5_to_be_generated.csv\")\n",
    "\n",
    "# new column 'fake_id'\n",
    "df['fake_id'] = ''\n",
    "\n",
    "# assign values to 'fake_id' based on row index intervals\n",
    "df.loc[df.index < 1263, 'fake_id'] = 'SD_fake_' + df['id']\n",
    "df.loc[(df.index >= 1263) & (df.index < 2505), 'fake_id'] = 'DL_fake_' + df['id']\n",
    "df.loc[df.index >= 2505, 'fake_id'] = 'GL_fake_' + df['id']\n",
    "\n",
    "# 'fake_id' column next to 'id' column\n",
    "df.insert(df.columns.get_loc('id') + 1, 'fake_id', df.pop('fake_id'))\n",
    "\n",
    "df.to_csv('/home/enriconello/DeepFakeDetection/Thesis/5_biased_detection/csv/validation/validation_class_1_2_3_5_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set the column pristine_image = 0 and generated_image = 1 for every row in validation_class_0_final csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/validation/validation_class_0_final.csv')\n",
    "\n",
    "# Swap the values of the two columns\n",
    "df['pristine_image'], df['generated_image'] = df['generated_image'], df['pristine_image']\n",
    "\n",
    "df.to_csv(\"csv/validation/validation_class_0_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/validation/validation_class_1_2_3_5_final.csv')\n",
    "\n",
    "# Swap the values of the two columns\n",
    "df['pristine_image'], df['generated_image'] = df['generated_image'], df['pristine_image']\n",
    "\n",
    "df.to_csv(\"csv/validation/validation_class_1_2_3_5_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now i need to add the column \"generated_caption\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fake_id</th>\n",
       "      <th>author</th>\n",
       "      <th>original_caption</th>\n",
       "      <th>generated_caption</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>6_way_label</th>\n",
       "      <th>pristine_image</th>\n",
       "      <th>generated_image</th>\n",
       "      <th>real_text</th>\n",
       "      <th>fakenews_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blkphy</td>\n",
       "      <td>SD_fake_blkphy</td>\n",
       "      <td>jespeland01</td>\n",
       "      <td>sitting on the edge of a cliff</td>\n",
       "      <td>a man sitting on a rock looking out over a valley</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c0e0xx</td>\n",
       "      <td>SD_fake_c0e0xx</td>\n",
       "      <td>Zirocket</td>\n",
       "      <td>this building beside the train station painted...</td>\n",
       "      <td>there are two trains that are on the tracks in...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cddj2g</td>\n",
       "      <td>SD_fake_cddj2g</td>\n",
       "      <td>courtysim</td>\n",
       "      <td>orourke says he and wife descended from slave ...</td>\n",
       "      <td>man in a white shirt and tie holding a microphone</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c9kpo8</td>\n",
       "      <td>SD_fake_c9kpo8</td>\n",
       "      <td>andrewgardnr</td>\n",
       "      <td>this is a phone screen under a microscope</td>\n",
       "      <td>a close up of a light that is shining brightly...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7u9y4x</td>\n",
       "      <td>SD_fake_7u9y4x</td>\n",
       "      <td>sisyphushaditsoeasy</td>\n",
       "      <td>students parents surprise school bus driver wi...</td>\n",
       "      <td>man in a green hat is hugging a woman</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id         fake_id               author  \\\n",
       "0  blkphy  SD_fake_blkphy          jespeland01   \n",
       "1  c0e0xx  SD_fake_c0e0xx             Zirocket   \n",
       "2  cddj2g  SD_fake_cddj2g            courtysim   \n",
       "3  c9kpo8  SD_fake_c9kpo8         andrewgardnr   \n",
       "4  7u9y4x  SD_fake_7u9y4x  sisyphushaditsoeasy   \n",
       "\n",
       "                                    original_caption  \\\n",
       "0                     sitting on the edge of a cliff   \n",
       "1  this building beside the train station painted...   \n",
       "2  orourke says he and wife descended from slave ...   \n",
       "3          this is a phone screen under a microscope   \n",
       "4  students parents surprise school bus driver wi...   \n",
       "\n",
       "                                   generated_caption  num_comments  \\\n",
       "0  a man sitting on a rock looking out over a valley           1.0   \n",
       "1  there are two trains that are on the tracks in...           0.0   \n",
       "2  man in a white shirt and tie holding a microphone           7.0   \n",
       "3  a close up of a light that is shining brightly...           6.0   \n",
       "4              man in a green hat is hugging a woman           0.0   \n",
       "\n",
       "   6_way_label  pristine_image  generated_image  real_text  fakenews_text  \n",
       "0            0               0                1          1              0  \n",
       "1            0               0                1          1              0  \n",
       "2            0               0                1          1              0  \n",
       "3            0               0                1          1              0  \n",
       "4            0               0                1          1              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_a = pd.read_csv(\"csv/validation/validation_class_0_final.csv\")\n",
    "df_b = pd.read_csv(\"csv/validation/validation_class_0_captioned.csv\")\n",
    "\n",
    "# Perform inner join on 'id' column\n",
    "df_c = pd.merge(df_a, df_b, on='id', how='inner')\n",
    "\n",
    "# Rename 'original_caption_x' to 'original_caption'\n",
    "df_c = df_c.rename(columns={'original_caption_x': 'original_caption'})\n",
    "\n",
    "df_c = df_c[['id', 'fake_id', 'author', 'original_caption', 'generated_caption', 'num_comments', '6_way_label', 'pristine_image', 'generated_image', 'real_text', 'fakenews_text']]\n",
    "\n",
    "df_c.to_csv(\"csv/validation/validation_class_0_final.csv\", index=False)\n",
    "df_c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fake_id</th>\n",
       "      <th>author</th>\n",
       "      <th>original_caption</th>\n",
       "      <th>generated_caption</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>6_way_label</th>\n",
       "      <th>pristine_image</th>\n",
       "      <th>generated_image</th>\n",
       "      <th>real_text</th>\n",
       "      <th>fakenews_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9hypej</td>\n",
       "      <td>SD_fake_9hypej</td>\n",
       "      <td>Yessod</td>\n",
       "      <td>i cannot believe this wasnt done on purpose</td>\n",
       "      <td>there are two trash bins that are sitting on t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c94gks</td>\n",
       "      <td>SD_fake_c94gks</td>\n",
       "      <td>xyzzyx13</td>\n",
       "      <td>jumping dolphin at moonlight</td>\n",
       "      <td>there is a drawing of a dolphin on a tile wall</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4905w5</td>\n",
       "      <td>SD_fake_4905w5</td>\n",
       "      <td>rexlibris</td>\n",
       "      <td>avec le agissez algerian war of independence s</td>\n",
       "      <td>a red poster with a hand holding a cigarette a...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>78fxbp</td>\n",
       "      <td>SD_fake_78fxbp</td>\n",
       "      <td>YouBecame</td>\n",
       "      <td>this man asked a simple question online that s...</td>\n",
       "      <td>a black and white photo of a man with a hand o...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9gfob0</td>\n",
       "      <td>SD_fake_9gfob0</td>\n",
       "      <td>RadioOphiuchi</td>\n",
       "      <td>hasselblad how quaint</td>\n",
       "      <td>there is a phone booth with a phone on the out...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id         fake_id         author  \\\n",
       "0  9hypej  SD_fake_9hypej         Yessod   \n",
       "1  c94gks  SD_fake_c94gks       xyzzyx13   \n",
       "2  4905w5  SD_fake_4905w5      rexlibris   \n",
       "3  78fxbp  SD_fake_78fxbp      YouBecame   \n",
       "4  9gfob0  SD_fake_9gfob0  RadioOphiuchi   \n",
       "\n",
       "                                    original_caption  \\\n",
       "0        i cannot believe this wasnt done on purpose   \n",
       "1                       jumping dolphin at moonlight   \n",
       "2     avec le agissez algerian war of independence s   \n",
       "3  this man asked a simple question online that s...   \n",
       "4                              hasselblad how quaint   \n",
       "\n",
       "                                   generated_caption  num_comments  \\\n",
       "0  there are two trash bins that are sitting on t...           0.0   \n",
       "1     there is a drawing of a dolphin on a tile wall           3.0   \n",
       "2  a red poster with a hand holding a cigarette a...           4.0   \n",
       "3  a black and white photo of a man with a hand o...          11.0   \n",
       "4  there is a phone booth with a phone on the out...           0.0   \n",
       "\n",
       "   6_way_label  pristine_image  generated_image  real_text  fakenews_text  \n",
       "0            2               0                1          0              1  \n",
       "1            2               0                1          0              1  \n",
       "2            5               0                1          0              1  \n",
       "3            5               0                1          0              1  \n",
       "4            1               0                1          0              1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_a = pd.read_csv(\"csv/validation/validation_class_1_2_3_5_final.csv\")\n",
    "df_b = pd.read_csv(\"csv/validation/validation_class_1_2_3_5_captioned.csv\")\n",
    "\n",
    "# Perform inner join on 'id' column\n",
    "df_c = pd.merge(df_a, df_b, on='id', how='inner')\n",
    "\n",
    "# Rename 'original_caption_x' to 'original_caption'\n",
    "df_c = df_c.rename(columns={'original_caption_x': 'original_caption'})\n",
    "\n",
    "df_c = df_c[['id', 'fake_id', 'author', 'original_caption', 'generated_caption', 'num_comments', '6_way_label', 'pristine_image', 'generated_image', 'real_text', 'fakenews_text']]\n",
    "\n",
    "df_c.to_csv(\"csv/validation/validation_class_1_2_3_5_final.csv\", index=False)\n",
    "df_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally merge validation_class_0_final and validation_class_1_2_3_5_final to create validation_generated_final csv that contains all the generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20454\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"csv/validation/validation_class_0_final.csv\")\n",
    "df3 = pd.read_csv(\"csv/validation/validation_class_0_already_generated.csv\")\n",
    "df2 = pd.read_csv(\"csv/validation/validation_class_1_2_3_5_final.csv\")\n",
    "df4 = pd.read_csv(\"csv/validation/validation_class_1_2_3_5_already_generated.csv\")\n",
    "\n",
    "# Concatenate DataFrames\n",
    "df_union = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "\n",
    "df_union.to_csv(\"csv/validation/validation_generated_final.csv\", index=False)\n",
    "print(len(df_union))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the cardinalities of the classes with the generation methods associated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for '6_way_label' 0:\n",
      "Starts with 'SD': 4000\n",
      "Starts with 'GL': 3490\n",
      "Starts with 'DL': 4000\n",
      "\n",
      "Counts for '6_way_label' 1:\n",
      "Starts with 'SD': 595\n",
      "Starts with 'GL': 561\n",
      "Starts with 'DL': 581\n",
      "\n",
      "Counts for '6_way_label' 2:\n",
      "Starts with 'SD': 1834\n",
      "Starts with 'GL': 1851\n",
      "Starts with 'DL': 1861\n",
      "\n",
      "Counts for '6_way_label' 3:\n",
      "Starts with 'SD': 206\n",
      "Starts with 'GL': 193\n",
      "Starts with 'DL': 199\n",
      "\n",
      "Counts for '6_way_label' 5:\n",
      "Starts with 'SD': 365\n",
      "Starts with 'GL': 359\n",
      "Starts with 'DL': 359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('csv/validation/validation_generated_final.csv')\n",
    "\n",
    "# Group the DataFrame by '6_way_label'\n",
    "grouped = df.groupby('6_way_label')\n",
    "\n",
    "# Iterate over each group\n",
    "for label, group in grouped:\n",
    "    # Filter the group where 'id' column starts with \"SD\", \"GL\", or \"DL\"\n",
    "    sd_count = len(group[group['fake_id'].str.startswith('SD')])\n",
    "    gl_count = len(group[group['fake_id'].str.startswith('GL')])\n",
    "    dl_count = len(group[group['fake_id'].str.startswith('DL')])\n",
    "    \n",
    "    # Print counts for each value of '6_way_label'\n",
    "    print(f\"Counts for '6_way_label' {label}:\")\n",
    "    print(f\"Starts with 'SD': {sd_count}\")\n",
    "    print(f\"Starts with 'GL': {gl_count}\")\n",
    "    print(f\"Starts with 'DL': {dl_count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now i need to merge validation_class_0_pristine and validation_class_1_2_3_5_pristine to make validation_pristine_final csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20455\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"csv/validation/validation_class_0_pristine.csv\")\n",
    "df2 = pd.read_csv(\"csv/validation/validation_class_1_2_3_5_pristine.csv\")\n",
    "\n",
    "# Concatenate DataFrames\n",
    "df_union = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "df_union.to_csv(\"csv/validation/validation_pristine_final.csv\", index=False)\n",
    "print(len(df_union))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally i need to merge the validation_pristine_final with validation_generated_final to generated validation csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40909\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_a = pd.read_csv('csv/validation/validation_generated_final.csv')\n",
    "df_b = pd.read_csv('csv/validation/validation_pristine_final.csv')\n",
    "\n",
    "# Add missing columns from A to B with empty values\n",
    "for column in df_a.columns:\n",
    "    if column not in df_b.columns:\n",
    "        df_b[column] = ''\n",
    "\n",
    "# Concatenate dataframes\n",
    "df_c = pd.concat([df_a, df_b])\n",
    "\n",
    "print(len(df_c))\n",
    "df_c.to_csv('csv/validation/validation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the images are already inside the images folder because the validation was \"with duplicates\", so also the real samples were maintained together with the fake ones. So for validation i should just add to the images folder the new generated images (those inside validation/class_0 and validation/class_1_2_3_5), and also duplicate the rows of the new generated images in the csv for the real samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching images: 40909\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the 'id' column from the CSV file into a set\n",
    "csv_ids = set(pd.read_csv(\"csv/validation/validation.csv\")['id'])\n",
    "\n",
    "# Get a list of all the image filenames in the folder\n",
    "folder_path = \"/home/enriconello/DeepFakeDetection/dataset\"\n",
    "folder_images = set(filename.split('.')[0] for filename in os.listdir(folder_path))\n",
    "\n",
    "# Find the intersection to identify matching images\n",
    "matching_images = csv_ids.intersection(folder_images)\n",
    "\n",
    "# Count the number of matching images\n",
    "num_matching_images = len(matching_images)\n",
    "\n",
    "print(\"Number of matching images:\", num_matching_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
